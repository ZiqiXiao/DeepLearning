{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This file shows how the torch_model works. We go through all the functions in torch_model file so that understand what is going on. And we could make some adjustment in the future while the actual scene."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare the data.\n",
    "We use the prepare data function in data_utils file, which is going through step by step in \"pytorch_example_data_utils_sbs\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.data_utils import prepare_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult.csv')\n",
    "df['income_label'] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "wide_cols = ['age','hours-per-week','education', 'relationship','workclass',\n",
    "             'occupation','native-country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native-country', 'occupation'])\n",
    "embeddings_cols = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                   ('occupation',10),('native-country',10)]\n",
    "continuous_cols = [\"age\",\"hours-per-week\"]\n",
    "target = 'income_label'\n",
    "method = 'logistic'\n",
    "\n",
    "wd_dataset = prepare_data(df, wide_cols,crossed_cols,embeddings_cols,continuous_cols,target,scale=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'train_dataset': train_dataset(wide=array([[23, 40,  0, ...,  0,  0,  0],\n        [23, 40,  0, ...,  0,  0,  0],\n        [42, 50,  0, ...,  0,  0,  0],\n        ...,\n        [26, 10,  0, ...,  0,  0,  0],\n        [42, 66,  0, ...,  0,  0,  0],\n        [32, 15,  0, ...,  0,  0,  0]]), deep=array([[ 3.        ,  5.        ,  0.        , ...,  0.        ,\n         -1.14100392, -0.03408696],\n        [11.        ,  0.        ,  0.        , ...,  0.        ,\n         -1.14100392, -0.03408696],\n        [ 9.        ,  1.        ,  0.        , ..., 27.        ,\n          0.24480847,  0.77292975],\n        ...,\n        [ 7.        ,  2.        ,  0.        , ...,  3.        ,\n         -0.92219144, -2.45513709],\n        [ 3.        ,  1.        ,  3.        , ...,  0.        ,\n          0.24480847,  2.06415648],\n        [ 3.        ,  0.        ,  0.        , ...,  0.        ,\n         -0.48456647, -2.05162874]]), labels=array([0, 0, 1, ..., 0, 0, 0])),\n 'test_dataset': test_dataset(wide=array([[24, 40,  0, ...,  0,  0,  0],\n        [23, 42,  0, ...,  0,  0,  0],\n        [20, 25,  0, ...,  0,  0,  0],\n        ...,\n        [67, 25,  0, ...,  0,  0,  0],\n        [42, 40,  0, ...,  0,  0,  0],\n        [28, 40,  0, ...,  0,  0,  0]]), deep=array([[ 1.        ,  0.        ,  0.        , ...,  0.        ,\n         -1.06806643, -0.03408696],\n        [ 7.        ,  0.        ,  0.        , ...,  0.        ,\n         -1.14100392,  0.12731638],\n        [ 1.        ,  0.        ,  2.        , ...,  0.        ,\n         -1.3598164 , -1.24461203],\n        ...,\n        [ 3.        ,  2.        ,  0.        , ...,  0.        ,\n          2.06824582, -1.24461203],\n        [ 3.        ,  4.        ,  1.        , ...,  0.        ,\n          0.24480847, -0.03408696],\n        [ 1.        ,  3.        ,  0.        , ...,  0.        ,\n         -0.77631645, -0.03408696]]), labels=array([0, 0, 0, ..., 0, 1, 0])),\n 'embeddings_input': [('education', 16, 10),\n  ('relationship', 6, 8),\n  ('occupation', 15, 10),\n  ('workclass', 9, 10),\n  ('native-country', 42, 10)],\n 'deep_column_idx': {'education': 0,\n  'relationship': 1,\n  'workclass': 2,\n  'occupation': 3,\n  'native-country': 4,\n  'age': 5,\n  'hours-per-week': 6},\n 'encoding_dict': {'education': {'11th': 0,\n   'HS-grad': 1,\n   'Assoc-acdm': 2,\n   'Some-college': 3,\n   '10th': 4,\n   'Prof-school': 5,\n   '7th-8th': 6,\n   'Bachelors': 7,\n   'Masters': 8,\n   'Doctorate': 9,\n   '5th-6th': 10,\n   'Assoc-voc': 11,\n   '9th': 12,\n   '12th': 13,\n   '1st-4th': 14,\n   'Preschool': 15},\n  'relationship': {'Own-child': 0,\n   'Husband': 1,\n   'Not-in-family': 2,\n   'Unmarried': 3,\n   'Wife': 4,\n   'Other-relative': 5},\n  'occupation': {'Machine-op-inspct': 0,\n   'Farming-fishing': 1,\n   'Protective-serv': 2,\n   '?': 3,\n   'Other-service': 4,\n   'Prof-specialty': 5,\n   'Craft-repair': 6,\n   'Adm-clerical': 7,\n   'Exec-managerial': 8,\n   'Tech-support': 9,\n   'Sales': 10,\n   'Priv-house-serv': 11,\n   'Transport-moving': 12,\n   'Handlers-cleaners': 13,\n   'Armed-Forces': 14},\n  'workclass': {'Private': 0,\n   'Local-gov': 1,\n   '?': 2,\n   'Self-emp-not-inc': 3,\n   'Federal-gov': 4,\n   'State-gov': 5,\n   'Self-emp-inc': 6,\n   'Without-pay': 7,\n   'Never-worked': 8},\n  'native-country': {'United-States': 0,\n   '?': 1,\n   'Peru': 2,\n   'Guatemala': 3,\n   'Mexico': 4,\n   'Dominican-Republic': 5,\n   'Ireland': 6,\n   'Germany': 7,\n   'Philippines': 8,\n   'Thailand': 9,\n   'Haiti': 10,\n   'El-Salvador': 11,\n   'Puerto-Rico': 12,\n   'Vietnam': 13,\n   'South': 14,\n   'Columbia': 15,\n   'Japan': 16,\n   'India': 17,\n   'Cambodia': 18,\n   'Poland': 19,\n   'Laos': 20,\n   'England': 21,\n   'Cuba': 22,\n   'Taiwan': 23,\n   'Italy': 24,\n   'Canada': 25,\n   'Portugal': 26,\n   'China': 27,\n   'Nicaragua': 28,\n   'Honduras': 29,\n   'Iran': 30,\n   'Scotland': 31,\n   'Jamaica': 32,\n   'Ecuador': 33,\n   'Yugoslavia': 34,\n   'Hungary': 35,\n   'Hong': 36,\n   'Greece': 37,\n   'Trinadad&Tobago': 38,\n   'Outlying-US(Guam-USVI-etc)': 39,\n   'France': 40,\n   'Holand-Netherlands': 41}}}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build the Wide part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=798, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class  = 1\n",
    "wide_part = nn.Linear(wide_dim, n_class)\n",
    "\n",
    "print(wide_part)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    \"\"\"\n",
    "    Wide-side consists in simply in \"pluging\" the features into the output neuron(s)\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    wide_dim: int. Number of features per observation\n",
    "    method  : str. Regression, logistic or multiclass\n",
    "    n_class : int. number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self, wide_dim, n_class):\n",
    "\n",
    "        super(Wide, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.linear = nn.Linear(self.wide_dim, self.n_class)\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        out = torch.sigmoid(self.linear(X))\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide(\n",
      "  (linear): Linear(in_features=798, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class  = 1\n",
    "wide_model = Wide(wide_dim, n_class)\n",
    "print(wide_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(34189, 1)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset['train_dataset'].labels.reshape(-1, 1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0, 23, 40, ...,  0,  0,  0],\n       [ 0, 23, 40, ...,  0,  0,  0],\n       [ 1, 42, 50, ...,  0,  0,  0],\n       ...,\n       [ 0, 26, 10, ...,  0,  0,  0],\n       [ 0, 42, 66, ...,  0,  0,  0],\n       [ 0, 32, 15, ...,  0,  0,  0]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].wide])\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoziqi/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1000, Loss: 0.197, accuracy: 0.8372\n",
      "Epoch 2 of 1000, Loss: 0.218, accuracy: 0.8364\n",
      "Epoch 3 of 1000, Loss: 0.344, accuracy: 0.8375\n",
      "Epoch 4 of 1000, Loss: 0.389, accuracy: 0.8369\n",
      "Epoch 5 of 1000, Loss: 0.513, accuracy: 0.8368\n",
      "Epoch 6 of 1000, Loss: 0.388, accuracy: 0.8357\n",
      "Epoch 7 of 1000, Loss: 0.135, accuracy: 0.8382\n",
      "Epoch 8 of 1000, Loss: 0.295, accuracy: 0.8373\n",
      "Epoch 9 of 1000, Loss: 0.246, accuracy: 0.8374\n",
      "Epoch 10 of 1000, Loss: 0.507, accuracy: 0.837\n",
      "Epoch 11 of 1000, Loss: 0.308, accuracy: 0.8374\n",
      "Epoch 12 of 1000, Loss: 0.15, accuracy: 0.838\n",
      "Epoch 13 of 1000, Loss: 0.358, accuracy: 0.8373\n",
      "Epoch 14 of 1000, Loss: 0.488, accuracy: 0.8366\n",
      "Epoch 15 of 1000, Loss: 0.376, accuracy: 0.8372\n",
      "Epoch 16 of 1000, Loss: 0.146, accuracy: 0.8378\n",
      "Epoch 17 of 1000, Loss: 0.319, accuracy: 0.8381\n",
      "Epoch 18 of 1000, Loss: 0.205, accuracy: 0.8382\n",
      "Epoch 19 of 1000, Loss: 0.166, accuracy: 0.8371\n",
      "Epoch 20 of 1000, Loss: 0.565, accuracy: 0.837\n",
      "Epoch 21 of 1000, Loss: 0.323, accuracy: 0.8385\n",
      "Epoch 22 of 1000, Loss: 0.353, accuracy: 0.838\n",
      "Epoch 23 of 1000, Loss: 0.383, accuracy: 0.8376\n",
      "Epoch 24 of 1000, Loss: 0.313, accuracy: 0.8384\n",
      "Epoch 25 of 1000, Loss: 0.545, accuracy: 0.8381\n",
      "Epoch 26 of 1000, Loss: 0.532, accuracy: 0.8386\n",
      "Epoch 27 of 1000, Loss: 0.313, accuracy: 0.8381\n",
      "Epoch 28 of 1000, Loss: 0.317, accuracy: 0.8367\n",
      "Epoch 29 of 1000, Loss: 0.47, accuracy: 0.8378\n",
      "Epoch 30 of 1000, Loss: 0.322, accuracy: 0.8371\n",
      "Epoch 31 of 1000, Loss: 0.52, accuracy: 0.8378\n",
      "Epoch 32 of 1000, Loss: 0.137, accuracy: 0.8374\n",
      "Epoch 33 of 1000, Loss: 0.471, accuracy: 0.8378\n",
      "Epoch 34 of 1000, Loss: 0.402, accuracy: 0.8383\n",
      "Epoch 35 of 1000, Loss: 0.492, accuracy: 0.8377\n",
      "Epoch 36 of 1000, Loss: 0.236, accuracy: 0.838\n",
      "Epoch 37 of 1000, Loss: 0.225, accuracy: 0.8382\n",
      "Epoch 38 of 1000, Loss: 0.163, accuracy: 0.8386\n",
      "Epoch 39 of 1000, Loss: 0.196, accuracy: 0.8369\n",
      "Epoch 40 of 1000, Loss: 0.204, accuracy: 0.8381\n",
      "Epoch 41 of 1000, Loss: 0.492, accuracy: 0.8383\n",
      "Epoch 42 of 1000, Loss: 0.275, accuracy: 0.8372\n",
      "Epoch 43 of 1000, Loss: 0.571, accuracy: 0.8387\n",
      "Epoch 44 of 1000, Loss: 0.5, accuracy: 0.8377\n",
      "Epoch 45 of 1000, Loss: 0.31, accuracy: 0.8382\n",
      "Epoch 46 of 1000, Loss: 0.56, accuracy: 0.8379\n",
      "Epoch 47 of 1000, Loss: 0.233, accuracy: 0.8373\n",
      "Epoch 48 of 1000, Loss: 0.116, accuracy: 0.8377\n",
      "Epoch 49 of 1000, Loss: 0.167, accuracy: 0.8387\n",
      "Epoch 50 of 1000, Loss: 0.288, accuracy: 0.8381\n",
      "Epoch 51 of 1000, Loss: 0.247, accuracy: 0.8385\n",
      "Epoch 52 of 1000, Loss: 0.338, accuracy: 0.8376\n",
      "Epoch 53 of 1000, Loss: 0.176, accuracy: 0.8378\n",
      "Epoch 54 of 1000, Loss: 0.306, accuracy: 0.8381\n",
      "Epoch 55 of 1000, Loss: 0.363, accuracy: 0.8389\n",
      "Epoch 56 of 1000, Loss: 0.824, accuracy: 0.8392\n",
      "Epoch 57 of 1000, Loss: 0.256, accuracy: 0.8378\n",
      "Epoch 58 of 1000, Loss: 0.274, accuracy: 0.8377\n",
      "Epoch 59 of 1000, Loss: 0.435, accuracy: 0.8379\n",
      "Epoch 60 of 1000, Loss: 0.223, accuracy: 0.8376\n",
      "Epoch 61 of 1000, Loss: 0.189, accuracy: 0.8378\n",
      "Epoch 62 of 1000, Loss: 0.558, accuracy: 0.8378\n",
      "Epoch 63 of 1000, Loss: 0.292, accuracy: 0.839\n",
      "Epoch 64 of 1000, Loss: 0.321, accuracy: 0.8373\n",
      "Epoch 65 of 1000, Loss: 0.415, accuracy: 0.8376\n",
      "Epoch 66 of 1000, Loss: 0.236, accuracy: 0.8388\n",
      "Epoch 67 of 1000, Loss: 0.184, accuracy: 0.8375\n",
      "Epoch 68 of 1000, Loss: 0.572, accuracy: 0.8382\n",
      "Epoch 69 of 1000, Loss: 0.367, accuracy: 0.8383\n",
      "Epoch 70 of 1000, Loss: 0.341, accuracy: 0.8385\n",
      "Epoch 71 of 1000, Loss: 0.517, accuracy: 0.8387\n",
      "Epoch 72 of 1000, Loss: 0.369, accuracy: 0.8385\n",
      "Epoch 73 of 1000, Loss: 0.34, accuracy: 0.8379\n",
      "Epoch 74 of 1000, Loss: 0.282, accuracy: 0.838\n",
      "Epoch 75 of 1000, Loss: 0.236, accuracy: 0.8395\n",
      "Epoch 76 of 1000, Loss: 0.265, accuracy: 0.8388\n",
      "Epoch 77 of 1000, Loss: 0.469, accuracy: 0.839\n",
      "Epoch 78 of 1000, Loss: 0.321, accuracy: 0.8389\n",
      "Epoch 79 of 1000, Loss: 0.244, accuracy: 0.8386\n",
      "Epoch 80 of 1000, Loss: 0.361, accuracy: 0.8384\n",
      "Epoch 81 of 1000, Loss: 0.334, accuracy: 0.8375\n",
      "Epoch 82 of 1000, Loss: 0.088, accuracy: 0.8386\n",
      "Epoch 83 of 1000, Loss: 0.333, accuracy: 0.8391\n",
      "Epoch 84 of 1000, Loss: 0.377, accuracy: 0.8387\n",
      "Epoch 85 of 1000, Loss: 0.186, accuracy: 0.838\n",
      "Epoch 86 of 1000, Loss: 0.338, accuracy: 0.838\n",
      "Epoch 87 of 1000, Loss: 0.222, accuracy: 0.8388\n",
      "Epoch 88 of 1000, Loss: 0.198, accuracy: 0.8382\n",
      "Epoch 89 of 1000, Loss: 0.56, accuracy: 0.8387\n",
      "Epoch 90 of 1000, Loss: 0.399, accuracy: 0.8393\n",
      "Epoch 91 of 1000, Loss: 0.447, accuracy: 0.8391\n",
      "Epoch 92 of 1000, Loss: 0.546, accuracy: 0.839\n",
      "Epoch 93 of 1000, Loss: 0.191, accuracy: 0.8378\n",
      "Epoch 94 of 1000, Loss: 0.189, accuracy: 0.8388\n",
      "Epoch 95 of 1000, Loss: 0.397, accuracy: 0.8388\n",
      "Epoch 96 of 1000, Loss: 0.218, accuracy: 0.8379\n",
      "Epoch 97 of 1000, Loss: 0.241, accuracy: 0.8382\n",
      "Epoch 98 of 1000, Loss: 0.231, accuracy: 0.839\n",
      "Epoch 99 of 1000, Loss: 0.37, accuracy: 0.8383\n",
      "Epoch 100 of 1000, Loss: 0.31, accuracy: 0.8381\n",
      "Epoch 101 of 1000, Loss: 0.178, accuracy: 0.8388\n",
      "Epoch 102 of 1000, Loss: 0.354, accuracy: 0.8392\n",
      "Epoch 103 of 1000, Loss: 0.182, accuracy: 0.8387\n",
      "Epoch 104 of 1000, Loss: 0.472, accuracy: 0.8394\n",
      "Epoch 105 of 1000, Loss: 0.528, accuracy: 0.8388\n",
      "Epoch 106 of 1000, Loss: 0.171, accuracy: 0.8385\n",
      "Epoch 107 of 1000, Loss: 0.124, accuracy: 0.8391\n",
      "Epoch 108 of 1000, Loss: 0.309, accuracy: 0.8389\n",
      "Epoch 109 of 1000, Loss: 0.308, accuracy: 0.8385\n",
      "Epoch 110 of 1000, Loss: 0.446, accuracy: 0.8384\n",
      "Epoch 111 of 1000, Loss: 0.316, accuracy: 0.8386\n",
      "Epoch 112 of 1000, Loss: 0.328, accuracy: 0.8387\n",
      "Epoch 113 of 1000, Loss: 0.346, accuracy: 0.8394\n",
      "Epoch 114 of 1000, Loss: 0.342, accuracy: 0.8398\n",
      "Epoch 115 of 1000, Loss: 0.338, accuracy: 0.8385\n",
      "Epoch 116 of 1000, Loss: 0.148, accuracy: 0.8381\n",
      "Epoch 117 of 1000, Loss: 0.346, accuracy: 0.8387\n",
      "Epoch 118 of 1000, Loss: 0.282, accuracy: 0.8389\n",
      "Epoch 119 of 1000, Loss: 0.129, accuracy: 0.8385\n",
      "Epoch 120 of 1000, Loss: 0.465, accuracy: 0.8379\n",
      "Epoch 121 of 1000, Loss: 0.256, accuracy: 0.8393\n",
      "Epoch 122 of 1000, Loss: 0.338, accuracy: 0.8387\n",
      "Epoch 123 of 1000, Loss: 0.165, accuracy: 0.8386\n",
      "Epoch 124 of 1000, Loss: 0.055, accuracy: 0.8393\n",
      "Epoch 125 of 1000, Loss: 0.4, accuracy: 0.838\n",
      "Epoch 126 of 1000, Loss: 0.259, accuracy: 0.838\n",
      "Epoch 127 of 1000, Loss: 0.264, accuracy: 0.8376\n",
      "Epoch 128 of 1000, Loss: 0.194, accuracy: 0.8383\n",
      "Epoch 129 of 1000, Loss: 0.22, accuracy: 0.8381\n",
      "Epoch 130 of 1000, Loss: 0.364, accuracy: 0.8392\n",
      "Epoch 131 of 1000, Loss: 0.365, accuracy: 0.8385\n",
      "Epoch 132 of 1000, Loss: 0.204, accuracy: 0.839\n",
      "Epoch 133 of 1000, Loss: 0.29, accuracy: 0.8397\n",
      "Epoch 134 of 1000, Loss: 0.234, accuracy: 0.8392\n",
      "Epoch 135 of 1000, Loss: 0.557, accuracy: 0.8378\n",
      "Epoch 136 of 1000, Loss: 0.599, accuracy: 0.8395\n",
      "Epoch 137 of 1000, Loss: 0.29, accuracy: 0.838\n",
      "Epoch 138 of 1000, Loss: 0.676, accuracy: 0.8381\n",
      "Epoch 139 of 1000, Loss: 0.552, accuracy: 0.8395\n",
      "Epoch 140 of 1000, Loss: 0.38, accuracy: 0.8379\n",
      "Epoch 141 of 1000, Loss: 0.188, accuracy: 0.8383\n",
      "Epoch 142 of 1000, Loss: 0.342, accuracy: 0.8391\n",
      "Epoch 143 of 1000, Loss: 0.462, accuracy: 0.8381\n",
      "Epoch 144 of 1000, Loss: 0.167, accuracy: 0.8392\n",
      "Epoch 145 of 1000, Loss: 0.426, accuracy: 0.8391\n",
      "Epoch 146 of 1000, Loss: 0.208, accuracy: 0.8381\n",
      "Epoch 147 of 1000, Loss: 0.29, accuracy: 0.8385\n",
      "Epoch 148 of 1000, Loss: 0.405, accuracy: 0.838\n",
      "Epoch 149 of 1000, Loss: 0.367, accuracy: 0.839\n",
      "Epoch 150 of 1000, Loss: 0.41, accuracy: 0.839\n",
      "Epoch 151 of 1000, Loss: 0.413, accuracy: 0.8387\n",
      "Epoch 152 of 1000, Loss: 0.36, accuracy: 0.8388\n",
      "Epoch 153 of 1000, Loss: 0.723, accuracy: 0.8382\n",
      "Epoch 154 of 1000, Loss: 0.319, accuracy: 0.8384\n",
      "Epoch 155 of 1000, Loss: 0.288, accuracy: 0.8385\n",
      "Epoch 156 of 1000, Loss: 0.263, accuracy: 0.8379\n",
      "Epoch 157 of 1000, Loss: 0.398, accuracy: 0.8383\n",
      "Epoch 158 of 1000, Loss: 0.273, accuracy: 0.8382\n",
      "Epoch 159 of 1000, Loss: 0.646, accuracy: 0.8383\n",
      "Epoch 160 of 1000, Loss: 0.466, accuracy: 0.8392\n",
      "Epoch 161 of 1000, Loss: 0.339, accuracy: 0.8383\n",
      "Epoch 162 of 1000, Loss: 0.142, accuracy: 0.8386\n",
      "Epoch 163 of 1000, Loss: 0.584, accuracy: 0.8385\n",
      "Epoch 164 of 1000, Loss: 0.292, accuracy: 0.839\n",
      "Epoch 165 of 1000, Loss: 0.266, accuracy: 0.8392\n",
      "Epoch 166 of 1000, Loss: 0.31, accuracy: 0.8392\n",
      "Epoch 167 of 1000, Loss: 0.375, accuracy: 0.8389\n",
      "Epoch 168 of 1000, Loss: 0.609, accuracy: 0.8387\n",
      "Epoch 169 of 1000, Loss: 0.172, accuracy: 0.8388\n",
      "Epoch 170 of 1000, Loss: 0.224, accuracy: 0.8378\n",
      "Epoch 171 of 1000, Loss: 0.147, accuracy: 0.839\n",
      "Epoch 172 of 1000, Loss: 0.363, accuracy: 0.8397\n",
      "Epoch 173 of 1000, Loss: 0.514, accuracy: 0.8384\n",
      "Epoch 174 of 1000, Loss: 0.479, accuracy: 0.8384\n",
      "Epoch 175 of 1000, Loss: 0.309, accuracy: 0.8388\n",
      "Epoch 176 of 1000, Loss: 0.374, accuracy: 0.838\n",
      "Epoch 177 of 1000, Loss: 0.488, accuracy: 0.839\n",
      "Epoch 178 of 1000, Loss: 0.548, accuracy: 0.8386\n",
      "Epoch 179 of 1000, Loss: 0.156, accuracy: 0.8399\n",
      "Epoch 180 of 1000, Loss: 0.197, accuracy: 0.8388\n",
      "Epoch 181 of 1000, Loss: 0.174, accuracy: 0.8391\n",
      "Epoch 182 of 1000, Loss: 0.267, accuracy: 0.8393\n",
      "Epoch 183 of 1000, Loss: 0.25, accuracy: 0.8389\n",
      "Epoch 184 of 1000, Loss: 0.342, accuracy: 0.8385\n",
      "Epoch 185 of 1000, Loss: 0.365, accuracy: 0.8394\n",
      "Epoch 186 of 1000, Loss: 0.292, accuracy: 0.8379\n",
      "Epoch 187 of 1000, Loss: 0.191, accuracy: 0.8382\n",
      "Epoch 188 of 1000, Loss: 0.265, accuracy: 0.8389\n",
      "Epoch 189 of 1000, Loss: 0.307, accuracy: 0.8385\n",
      "Epoch 190 of 1000, Loss: 0.293, accuracy: 0.8383\n",
      "Epoch 191 of 1000, Loss: 0.327, accuracy: 0.8383\n",
      "Epoch 192 of 1000, Loss: 0.409, accuracy: 0.8378\n",
      "Epoch 193 of 1000, Loss: 0.257, accuracy: 0.8394\n",
      "Epoch 194 of 1000, Loss: 0.36, accuracy: 0.8378\n",
      "Epoch 195 of 1000, Loss: 0.519, accuracy: 0.8384\n",
      "Epoch 196 of 1000, Loss: 0.27, accuracy: 0.8389\n",
      "Epoch 197 of 1000, Loss: 0.371, accuracy: 0.8381\n",
      "Epoch 198 of 1000, Loss: 0.607, accuracy: 0.8391\n",
      "Epoch 199 of 1000, Loss: 0.289, accuracy: 0.8395\n",
      "Epoch 200 of 1000, Loss: 0.578, accuracy: 0.8389\n",
      "Epoch 201 of 1000, Loss: 0.343, accuracy: 0.8394\n",
      "Epoch 202 of 1000, Loss: 0.413, accuracy: 0.8389\n",
      "Epoch 203 of 1000, Loss: 0.208, accuracy: 0.8383\n",
      "Epoch 204 of 1000, Loss: 0.305, accuracy: 0.8398\n",
      "Epoch 205 of 1000, Loss: 0.268, accuracy: 0.8384\n",
      "Epoch 206 of 1000, Loss: 0.465, accuracy: 0.8384\n",
      "Epoch 207 of 1000, Loss: 0.246, accuracy: 0.8388\n",
      "Epoch 208 of 1000, Loss: 0.224, accuracy: 0.8381\n",
      "Epoch 209 of 1000, Loss: 0.2, accuracy: 0.8383\n",
      "Epoch 210 of 1000, Loss: 0.379, accuracy: 0.8389\n",
      "Epoch 211 of 1000, Loss: 0.108, accuracy: 0.8401\n",
      "Epoch 212 of 1000, Loss: 0.449, accuracy: 0.8393\n",
      "Epoch 213 of 1000, Loss: 0.266, accuracy: 0.8388\n",
      "Epoch 214 of 1000, Loss: 0.18, accuracy: 0.8385\n",
      "Epoch 215 of 1000, Loss: 0.27, accuracy: 0.838\n",
      "Epoch 216 of 1000, Loss: 0.313, accuracy: 0.8395\n",
      "Epoch 217 of 1000, Loss: 0.432, accuracy: 0.8388\n",
      "Epoch 218 of 1000, Loss: 0.559, accuracy: 0.8399\n",
      "Epoch 219 of 1000, Loss: 0.121, accuracy: 0.8391\n",
      "Epoch 220 of 1000, Loss: 0.563, accuracy: 0.8394\n",
      "Epoch 221 of 1000, Loss: 0.3, accuracy: 0.8385\n",
      "Epoch 222 of 1000, Loss: 0.154, accuracy: 0.839\n",
      "Epoch 223 of 1000, Loss: 0.275, accuracy: 0.838\n",
      "Epoch 224 of 1000, Loss: 0.234, accuracy: 0.8391\n",
      "Epoch 225 of 1000, Loss: 0.659, accuracy: 0.839\n",
      "Epoch 226 of 1000, Loss: 0.236, accuracy: 0.8392\n",
      "Epoch 227 of 1000, Loss: 0.43, accuracy: 0.8389\n",
      "Epoch 228 of 1000, Loss: 0.31, accuracy: 0.8386\n",
      "Epoch 229 of 1000, Loss: 0.302, accuracy: 0.8383\n",
      "Epoch 230 of 1000, Loss: 0.336, accuracy: 0.838\n",
      "Epoch 231 of 1000, Loss: 0.393, accuracy: 0.8396\n",
      "Epoch 232 of 1000, Loss: 0.242, accuracy: 0.838\n",
      "Epoch 233 of 1000, Loss: 0.41, accuracy: 0.8384\n",
      "Epoch 234 of 1000, Loss: 0.488, accuracy: 0.8382\n",
      "Epoch 235 of 1000, Loss: 0.473, accuracy: 0.8397\n",
      "Epoch 236 of 1000, Loss: 0.263, accuracy: 0.8397\n",
      "Epoch 237 of 1000, Loss: 0.166, accuracy: 0.8381\n",
      "Epoch 238 of 1000, Loss: 0.713, accuracy: 0.8383\n",
      "Epoch 239 of 1000, Loss: 0.443, accuracy: 0.838\n",
      "Epoch 240 of 1000, Loss: 0.161, accuracy: 0.8377\n",
      "Epoch 241 of 1000, Loss: 0.293, accuracy: 0.8387\n",
      "Epoch 242 of 1000, Loss: 0.26, accuracy: 0.8382\n",
      "Epoch 243 of 1000, Loss: 0.139, accuracy: 0.8382\n",
      "Epoch 244 of 1000, Loss: 0.394, accuracy: 0.8379\n",
      "Epoch 245 of 1000, Loss: 0.489, accuracy: 0.8388\n",
      "Epoch 246 of 1000, Loss: 0.323, accuracy: 0.838\n",
      "Epoch 247 of 1000, Loss: 0.927, accuracy: 0.8384\n",
      "Epoch 248 of 1000, Loss: 0.323, accuracy: 0.839\n",
      "Epoch 249 of 1000, Loss: 0.401, accuracy: 0.8384\n",
      "Epoch 250 of 1000, Loss: 0.291, accuracy: 0.8388\n",
      "Epoch 251 of 1000, Loss: 0.552, accuracy: 0.8401\n",
      "Epoch 252 of 1000, Loss: 0.328, accuracy: 0.8381\n",
      "Epoch 253 of 1000, Loss: 0.535, accuracy: 0.8381\n",
      "Epoch 254 of 1000, Loss: 0.211, accuracy: 0.8381\n",
      "Epoch 255 of 1000, Loss: 0.338, accuracy: 0.8384\n",
      "Epoch 256 of 1000, Loss: 0.389, accuracy: 0.8388\n",
      "Epoch 257 of 1000, Loss: 0.364, accuracy: 0.8385\n",
      "Epoch 258 of 1000, Loss: 0.151, accuracy: 0.8386\n",
      "Epoch 259 of 1000, Loss: 0.287, accuracy: 0.8374\n",
      "Epoch 260 of 1000, Loss: 0.248, accuracy: 0.8385\n",
      "Epoch 261 of 1000, Loss: 0.177, accuracy: 0.8387\n",
      "Epoch 262 of 1000, Loss: 0.448, accuracy: 0.8385\n",
      "Epoch 263 of 1000, Loss: 0.203, accuracy: 0.8392\n",
      "Epoch 264 of 1000, Loss: 0.235, accuracy: 0.839\n",
      "Epoch 265 of 1000, Loss: 0.281, accuracy: 0.839\n",
      "Epoch 266 of 1000, Loss: 0.329, accuracy: 0.8393\n",
      "Epoch 267 of 1000, Loss: 0.37, accuracy: 0.8392\n",
      "Epoch 268 of 1000, Loss: 0.284, accuracy: 0.8388\n",
      "Epoch 269 of 1000, Loss: 0.443, accuracy: 0.8389\n",
      "Epoch 270 of 1000, Loss: 0.365, accuracy: 0.8387\n",
      "Epoch 271 of 1000, Loss: 0.21, accuracy: 0.8384\n",
      "Epoch 272 of 1000, Loss: 0.149, accuracy: 0.8385\n",
      "Epoch 273 of 1000, Loss: 0.306, accuracy: 0.8393\n",
      "Epoch 274 of 1000, Loss: 0.654, accuracy: 0.8387\n",
      "Epoch 275 of 1000, Loss: 0.343, accuracy: 0.8369\n",
      "Epoch 276 of 1000, Loss: 0.483, accuracy: 0.8399\n",
      "Epoch 277 of 1000, Loss: 0.383, accuracy: 0.8387\n",
      "Epoch 278 of 1000, Loss: 0.12, accuracy: 0.8393\n",
      "Epoch 279 of 1000, Loss: 0.424, accuracy: 0.8392\n",
      "Epoch 280 of 1000, Loss: 0.424, accuracy: 0.839\n",
      "Epoch 281 of 1000, Loss: 0.175, accuracy: 0.8386\n",
      "Epoch 282 of 1000, Loss: 0.208, accuracy: 0.8381\n",
      "Epoch 283 of 1000, Loss: 0.171, accuracy: 0.8384\n",
      "Epoch 284 of 1000, Loss: 0.141, accuracy: 0.839\n",
      "Epoch 285 of 1000, Loss: 0.37, accuracy: 0.8389\n",
      "Epoch 286 of 1000, Loss: 0.278, accuracy: 0.8392\n",
      "Epoch 287 of 1000, Loss: 0.261, accuracy: 0.8382\n",
      "Epoch 288 of 1000, Loss: 0.364, accuracy: 0.8394\n",
      "Epoch 289 of 1000, Loss: 0.425, accuracy: 0.8396\n",
      "Epoch 290 of 1000, Loss: 0.741, accuracy: 0.8391\n",
      "Epoch 291 of 1000, Loss: 0.261, accuracy: 0.8386\n",
      "Epoch 292 of 1000, Loss: 0.548, accuracy: 0.8383\n",
      "Epoch 293 of 1000, Loss: 0.2, accuracy: 0.8387\n",
      "Epoch 294 of 1000, Loss: 0.398, accuracy: 0.8393\n",
      "Epoch 295 of 1000, Loss: 0.318, accuracy: 0.839\n",
      "Epoch 296 of 1000, Loss: 0.347, accuracy: 0.8376\n",
      "Epoch 297 of 1000, Loss: 0.175, accuracy: 0.8395\n",
      "Epoch 298 of 1000, Loss: 0.425, accuracy: 0.8395\n",
      "Epoch 299 of 1000, Loss: 0.549, accuracy: 0.8384\n",
      "Epoch 300 of 1000, Loss: 0.688, accuracy: 0.8379\n",
      "Epoch 301 of 1000, Loss: 0.258, accuracy: 0.839\n",
      "Epoch 302 of 1000, Loss: 0.284, accuracy: 0.8389\n",
      "Epoch 303 of 1000, Loss: 0.281, accuracy: 0.8389\n",
      "Epoch 304 of 1000, Loss: 0.558, accuracy: 0.8389\n",
      "Epoch 305 of 1000, Loss: 0.385, accuracy: 0.8395\n",
      "Epoch 306 of 1000, Loss: 0.329, accuracy: 0.8378\n",
      "Epoch 307 of 1000, Loss: 0.19, accuracy: 0.838\n",
      "Epoch 308 of 1000, Loss: 0.183, accuracy: 0.8388\n",
      "Epoch 309 of 1000, Loss: 0.33, accuracy: 0.8387\n",
      "Epoch 310 of 1000, Loss: 0.215, accuracy: 0.8387\n",
      "Epoch 311 of 1000, Loss: 0.392, accuracy: 0.8383\n",
      "Epoch 312 of 1000, Loss: 0.259, accuracy: 0.8387\n",
      "Epoch 313 of 1000, Loss: 0.073, accuracy: 0.8386\n",
      "Epoch 314 of 1000, Loss: 0.401, accuracy: 0.839\n",
      "Epoch 315 of 1000, Loss: 0.12, accuracy: 0.8389\n",
      "Epoch 316 of 1000, Loss: 0.321, accuracy: 0.838\n",
      "Epoch 317 of 1000, Loss: 0.295, accuracy: 0.8385\n",
      "Epoch 318 of 1000, Loss: 0.381, accuracy: 0.8391\n",
      "Epoch 319 of 1000, Loss: 0.306, accuracy: 0.8397\n",
      "Epoch 320 of 1000, Loss: 0.104, accuracy: 0.8397\n",
      "Epoch 321 of 1000, Loss: 0.322, accuracy: 0.8391\n",
      "Epoch 322 of 1000, Loss: 0.611, accuracy: 0.8381\n",
      "Epoch 323 of 1000, Loss: 0.392, accuracy: 0.8391\n",
      "Epoch 324 of 1000, Loss: 0.238, accuracy: 0.8388\n",
      "Epoch 325 of 1000, Loss: 0.28, accuracy: 0.8393\n",
      "Epoch 326 of 1000, Loss: 0.248, accuracy: 0.8387\n",
      "Epoch 327 of 1000, Loss: 0.733, accuracy: 0.8381\n",
      "Epoch 328 of 1000, Loss: 0.521, accuracy: 0.8391\n",
      "Epoch 329 of 1000, Loss: 0.362, accuracy: 0.8396\n",
      "Epoch 330 of 1000, Loss: 0.131, accuracy: 0.8389\n",
      "Epoch 331 of 1000, Loss: 0.32, accuracy: 0.8385\n",
      "Epoch 332 of 1000, Loss: 0.377, accuracy: 0.8393\n",
      "Epoch 333 of 1000, Loss: 0.338, accuracy: 0.8391\n",
      "Epoch 334 of 1000, Loss: 0.216, accuracy: 0.8381\n",
      "Epoch 335 of 1000, Loss: 0.297, accuracy: 0.8383\n",
      "Epoch 336 of 1000, Loss: 0.202, accuracy: 0.8392\n",
      "Epoch 337 of 1000, Loss: 0.312, accuracy: 0.838\n",
      "Epoch 338 of 1000, Loss: 0.321, accuracy: 0.8391\n",
      "Epoch 339 of 1000, Loss: 0.474, accuracy: 0.8387\n",
      "Epoch 340 of 1000, Loss: 0.6, accuracy: 0.8394\n",
      "Epoch 341 of 1000, Loss: 0.388, accuracy: 0.8385\n",
      "Epoch 342 of 1000, Loss: 0.456, accuracy: 0.839\n",
      "Epoch 343 of 1000, Loss: 0.132, accuracy: 0.839\n",
      "Epoch 344 of 1000, Loss: 0.191, accuracy: 0.8392\n",
      "Epoch 345 of 1000, Loss: 0.392, accuracy: 0.8397\n",
      "Epoch 346 of 1000, Loss: 0.132, accuracy: 0.8391\n",
      "Epoch 347 of 1000, Loss: 0.351, accuracy: 0.839\n",
      "Epoch 348 of 1000, Loss: 0.347, accuracy: 0.8385\n",
      "Epoch 349 of 1000, Loss: 0.161, accuracy: 0.8379\n",
      "Epoch 350 of 1000, Loss: 0.193, accuracy: 0.8383\n",
      "Epoch 351 of 1000, Loss: 0.338, accuracy: 0.8393\n",
      "Epoch 352 of 1000, Loss: 0.501, accuracy: 0.8384\n",
      "Epoch 353 of 1000, Loss: 0.328, accuracy: 0.8382\n",
      "Epoch 354 of 1000, Loss: 0.449, accuracy: 0.8387\n",
      "Epoch 355 of 1000, Loss: 0.45, accuracy: 0.8394\n",
      "Epoch 356 of 1000, Loss: 0.165, accuracy: 0.8381\n",
      "Epoch 357 of 1000, Loss: 0.168, accuracy: 0.8386\n",
      "Epoch 358 of 1000, Loss: 0.183, accuracy: 0.8387\n",
      "Epoch 359 of 1000, Loss: 0.526, accuracy: 0.8375\n",
      "Epoch 360 of 1000, Loss: 0.231, accuracy: 0.8395\n",
      "Epoch 361 of 1000, Loss: 0.648, accuracy: 0.8387\n",
      "Epoch 362 of 1000, Loss: 0.35, accuracy: 0.8387\n",
      "Epoch 363 of 1000, Loss: 0.726, accuracy: 0.8389\n",
      "Epoch 364 of 1000, Loss: 0.596, accuracy: 0.8387\n",
      "Epoch 365 of 1000, Loss: 0.182, accuracy: 0.8383\n",
      "Epoch 366 of 1000, Loss: 0.151, accuracy: 0.8391\n",
      "Epoch 367 of 1000, Loss: 0.298, accuracy: 0.8395\n",
      "Epoch 368 of 1000, Loss: 0.365, accuracy: 0.8385\n",
      "Epoch 369 of 1000, Loss: 0.234, accuracy: 0.8388\n",
      "Epoch 370 of 1000, Loss: 0.663, accuracy: 0.8389\n",
      "Epoch 371 of 1000, Loss: 0.435, accuracy: 0.8386\n",
      "Epoch 372 of 1000, Loss: 0.376, accuracy: 0.8393\n",
      "Epoch 373 of 1000, Loss: 0.357, accuracy: 0.8388\n",
      "Epoch 374 of 1000, Loss: 0.263, accuracy: 0.8395\n",
      "Epoch 375 of 1000, Loss: 0.386, accuracy: 0.8395\n",
      "Epoch 376 of 1000, Loss: 0.203, accuracy: 0.8393\n",
      "Epoch 377 of 1000, Loss: 0.264, accuracy: 0.8391\n",
      "Epoch 378 of 1000, Loss: 0.313, accuracy: 0.8385\n",
      "Epoch 379 of 1000, Loss: 0.46, accuracy: 0.838\n",
      "Epoch 380 of 1000, Loss: 0.126, accuracy: 0.839\n",
      "Epoch 381 of 1000, Loss: 0.209, accuracy: 0.8387\n",
      "Epoch 382 of 1000, Loss: 0.462, accuracy: 0.839\n",
      "Epoch 383 of 1000, Loss: 0.509, accuracy: 0.838\n",
      "Epoch 384 of 1000, Loss: 0.362, accuracy: 0.8396\n",
      "Epoch 385 of 1000, Loss: 0.646, accuracy: 0.8393\n",
      "Epoch 386 of 1000, Loss: 0.145, accuracy: 0.8381\n",
      "Epoch 387 of 1000, Loss: 0.337, accuracy: 0.8388\n",
      "Epoch 388 of 1000, Loss: 0.421, accuracy: 0.839\n",
      "Epoch 389 of 1000, Loss: 0.299, accuracy: 0.8385\n",
      "Epoch 390 of 1000, Loss: 0.344, accuracy: 0.8383\n",
      "Epoch 391 of 1000, Loss: 0.283, accuracy: 0.8387\n",
      "Epoch 392 of 1000, Loss: 0.676, accuracy: 0.8383\n",
      "Epoch 393 of 1000, Loss: 0.425, accuracy: 0.8391\n",
      "Epoch 394 of 1000, Loss: 0.742, accuracy: 0.8387\n",
      "Epoch 395 of 1000, Loss: 0.22, accuracy: 0.8397\n",
      "Epoch 396 of 1000, Loss: 0.154, accuracy: 0.8386\n",
      "Epoch 397 of 1000, Loss: 0.148, accuracy: 0.8392\n",
      "Epoch 398 of 1000, Loss: 0.218, accuracy: 0.8374\n",
      "Epoch 399 of 1000, Loss: 0.351, accuracy: 0.838\n",
      "Epoch 400 of 1000, Loss: 0.102, accuracy: 0.839\n",
      "Epoch 401 of 1000, Loss: 0.417, accuracy: 0.839\n",
      "Epoch 402 of 1000, Loss: 0.668, accuracy: 0.8383\n",
      "Epoch 403 of 1000, Loss: 0.504, accuracy: 0.8384\n",
      "Epoch 404 of 1000, Loss: 0.229, accuracy: 0.8387\n",
      "Epoch 405 of 1000, Loss: 0.189, accuracy: 0.838\n",
      "Epoch 406 of 1000, Loss: 0.263, accuracy: 0.8385\n",
      "Epoch 407 of 1000, Loss: 0.293, accuracy: 0.8393\n",
      "Epoch 408 of 1000, Loss: 0.162, accuracy: 0.8386\n",
      "Epoch 409 of 1000, Loss: 0.513, accuracy: 0.8397\n",
      "Epoch 410 of 1000, Loss: 0.595, accuracy: 0.8389\n",
      "Epoch 411 of 1000, Loss: 0.415, accuracy: 0.8383\n",
      "Epoch 412 of 1000, Loss: 0.308, accuracy: 0.8387\n",
      "Epoch 413 of 1000, Loss: 0.416, accuracy: 0.8388\n",
      "Epoch 414 of 1000, Loss: 0.292, accuracy: 0.8385\n",
      "Epoch 415 of 1000, Loss: 0.298, accuracy: 0.8393\n",
      "Epoch 416 of 1000, Loss: 0.207, accuracy: 0.8382\n",
      "Epoch 417 of 1000, Loss: 0.945, accuracy: 0.8385\n",
      "Epoch 418 of 1000, Loss: 0.36, accuracy: 0.8382\n",
      "Epoch 419 of 1000, Loss: 0.421, accuracy: 0.8386\n",
      "Epoch 420 of 1000, Loss: 0.17, accuracy: 0.8383\n",
      "Epoch 421 of 1000, Loss: 0.297, accuracy: 0.8391\n",
      "Epoch 422 of 1000, Loss: 0.35, accuracy: 0.8387\n",
      "Epoch 423 of 1000, Loss: 0.647, accuracy: 0.8392\n",
      "Epoch 424 of 1000, Loss: 0.295, accuracy: 0.8377\n",
      "Epoch 425 of 1000, Loss: 0.452, accuracy: 0.8379\n",
      "Epoch 426 of 1000, Loss: 0.529, accuracy: 0.8383\n",
      "Epoch 427 of 1000, Loss: 0.491, accuracy: 0.8387\n",
      "Epoch 428 of 1000, Loss: 0.293, accuracy: 0.8388\n",
      "Epoch 429 of 1000, Loss: 0.143, accuracy: 0.8392\n",
      "Epoch 430 of 1000, Loss: 0.19, accuracy: 0.8385\n",
      "Epoch 431 of 1000, Loss: 0.447, accuracy: 0.8384\n",
      "Epoch 432 of 1000, Loss: 0.239, accuracy: 0.8395\n",
      "Epoch 433 of 1000, Loss: 0.392, accuracy: 0.8382\n",
      "Epoch 434 of 1000, Loss: 0.126, accuracy: 0.8385\n",
      "Epoch 435 of 1000, Loss: 0.131, accuracy: 0.839\n",
      "Epoch 436 of 1000, Loss: 0.418, accuracy: 0.8392\n",
      "Epoch 437 of 1000, Loss: 0.289, accuracy: 0.8393\n",
      "Epoch 438 of 1000, Loss: 0.153, accuracy: 0.8381\n",
      "Epoch 439 of 1000, Loss: 0.558, accuracy: 0.8391\n",
      "Epoch 440 of 1000, Loss: 0.588, accuracy: 0.8387\n",
      "Epoch 441 of 1000, Loss: 0.329, accuracy: 0.8381\n",
      "Epoch 442 of 1000, Loss: 0.348, accuracy: 0.8391\n",
      "Epoch 443 of 1000, Loss: 0.338, accuracy: 0.8394\n",
      "Epoch 444 of 1000, Loss: 0.477, accuracy: 0.8392\n",
      "Epoch 445 of 1000, Loss: 0.397, accuracy: 0.8394\n",
      "Epoch 446 of 1000, Loss: 0.144, accuracy: 0.8388\n",
      "Epoch 447 of 1000, Loss: 0.28, accuracy: 0.8392\n",
      "Epoch 448 of 1000, Loss: 0.202, accuracy: 0.8392\n",
      "Epoch 449 of 1000, Loss: 0.217, accuracy: 0.8379\n",
      "Epoch 450 of 1000, Loss: 0.19, accuracy: 0.8388\n",
      "Epoch 451 of 1000, Loss: 0.197, accuracy: 0.8399\n",
      "Epoch 452 of 1000, Loss: 0.249, accuracy: 0.8391\n",
      "Epoch 453 of 1000, Loss: 0.319, accuracy: 0.8383\n",
      "Epoch 454 of 1000, Loss: 0.289, accuracy: 0.8394\n",
      "Epoch 455 of 1000, Loss: 0.464, accuracy: 0.8392\n",
      "Epoch 456 of 1000, Loss: 0.375, accuracy: 0.839\n",
      "Epoch 457 of 1000, Loss: 0.216, accuracy: 0.8384\n",
      "Epoch 458 of 1000, Loss: 0.461, accuracy: 0.8397\n",
      "Epoch 459 of 1000, Loss: 0.222, accuracy: 0.8381\n",
      "Epoch 460 of 1000, Loss: 0.467, accuracy: 0.8385\n",
      "Epoch 461 of 1000, Loss: 0.301, accuracy: 0.8387\n",
      "Epoch 462 of 1000, Loss: 0.265, accuracy: 0.8388\n",
      "Epoch 463 of 1000, Loss: 0.351, accuracy: 0.8393\n",
      "Epoch 464 of 1000, Loss: 0.248, accuracy: 0.8378\n",
      "Epoch 465 of 1000, Loss: 0.182, accuracy: 0.839\n",
      "Epoch 466 of 1000, Loss: 0.793, accuracy: 0.8387\n",
      "Epoch 467 of 1000, Loss: 0.273, accuracy: 0.8384\n",
      "Epoch 468 of 1000, Loss: 0.234, accuracy: 0.8387\n",
      "Epoch 469 of 1000, Loss: 0.383, accuracy: 0.8394\n",
      "Epoch 470 of 1000, Loss: 0.219, accuracy: 0.8395\n",
      "Epoch 471 of 1000, Loss: 0.783, accuracy: 0.8397\n",
      "Epoch 472 of 1000, Loss: 0.402, accuracy: 0.8389\n",
      "Epoch 473 of 1000, Loss: 0.186, accuracy: 0.839\n",
      "Epoch 474 of 1000, Loss: 0.355, accuracy: 0.8394\n",
      "Epoch 475 of 1000, Loss: 0.276, accuracy: 0.8388\n",
      "Epoch 476 of 1000, Loss: 0.233, accuracy: 0.8388\n",
      "Epoch 477 of 1000, Loss: 0.265, accuracy: 0.8394\n",
      "Epoch 478 of 1000, Loss: 0.354, accuracy: 0.8385\n",
      "Epoch 479 of 1000, Loss: 0.389, accuracy: 0.8386\n",
      "Epoch 480 of 1000, Loss: 0.348, accuracy: 0.8375\n",
      "Epoch 481 of 1000, Loss: 0.232, accuracy: 0.8381\n",
      "Epoch 482 of 1000, Loss: 0.395, accuracy: 0.8383\n",
      "Epoch 483 of 1000, Loss: 0.157, accuracy: 0.8378\n",
      "Epoch 484 of 1000, Loss: 0.531, accuracy: 0.8394\n",
      "Epoch 485 of 1000, Loss: 0.208, accuracy: 0.839\n",
      "Epoch 486 of 1000, Loss: 0.099, accuracy: 0.8385\n",
      "Epoch 487 of 1000, Loss: 0.34, accuracy: 0.8386\n",
      "Epoch 488 of 1000, Loss: 0.268, accuracy: 0.8394\n",
      "Epoch 489 of 1000, Loss: 0.184, accuracy: 0.8386\n",
      "Epoch 490 of 1000, Loss: 0.354, accuracy: 0.8401\n",
      "Epoch 491 of 1000, Loss: 0.298, accuracy: 0.8385\n",
      "Epoch 492 of 1000, Loss: 0.272, accuracy: 0.8394\n",
      "Epoch 493 of 1000, Loss: 0.235, accuracy: 0.839\n",
      "Epoch 494 of 1000, Loss: 0.204, accuracy: 0.8392\n",
      "Epoch 495 of 1000, Loss: 0.394, accuracy: 0.8392\n",
      "Epoch 496 of 1000, Loss: 0.665, accuracy: 0.8396\n",
      "Epoch 497 of 1000, Loss: 0.156, accuracy: 0.8391\n",
      "Epoch 498 of 1000, Loss: 0.636, accuracy: 0.8387\n",
      "Epoch 499 of 1000, Loss: 0.488, accuracy: 0.8383\n",
      "Epoch 500 of 1000, Loss: 0.733, accuracy: 0.8392\n",
      "Epoch 501 of 1000, Loss: 0.33, accuracy: 0.8397\n",
      "Epoch 502 of 1000, Loss: 0.507, accuracy: 0.8384\n",
      "Epoch 503 of 1000, Loss: 0.407, accuracy: 0.8389\n",
      "Epoch 504 of 1000, Loss: 0.634, accuracy: 0.8386\n",
      "Epoch 505 of 1000, Loss: 0.355, accuracy: 0.8392\n",
      "Epoch 506 of 1000, Loss: 0.228, accuracy: 0.8385\n",
      "Epoch 507 of 1000, Loss: 0.452, accuracy: 0.8382\n",
      "Epoch 508 of 1000, Loss: 0.182, accuracy: 0.8389\n",
      "Epoch 509 of 1000, Loss: 0.593, accuracy: 0.8376\n",
      "Epoch 510 of 1000, Loss: 0.285, accuracy: 0.8389\n",
      "Epoch 511 of 1000, Loss: 0.439, accuracy: 0.8386\n",
      "Epoch 512 of 1000, Loss: 0.314, accuracy: 0.8394\n",
      "Epoch 513 of 1000, Loss: 0.403, accuracy: 0.839\n",
      "Epoch 514 of 1000, Loss: 0.512, accuracy: 0.8392\n",
      "Epoch 515 of 1000, Loss: 0.33, accuracy: 0.839\n",
      "Epoch 516 of 1000, Loss: 0.428, accuracy: 0.8386\n",
      "Epoch 517 of 1000, Loss: 0.366, accuracy: 0.8387\n",
      "Epoch 518 of 1000, Loss: 0.187, accuracy: 0.8388\n",
      "Epoch 519 of 1000, Loss: 0.295, accuracy: 0.8385\n",
      "Epoch 520 of 1000, Loss: 0.295, accuracy: 0.8381\n",
      "Epoch 521 of 1000, Loss: 0.256, accuracy: 0.8382\n",
      "Epoch 522 of 1000, Loss: 0.972, accuracy: 0.839\n",
      "Epoch 523 of 1000, Loss: 0.267, accuracy: 0.839\n",
      "Epoch 524 of 1000, Loss: 0.515, accuracy: 0.8389\n",
      "Epoch 525 of 1000, Loss: 0.137, accuracy: 0.8386\n",
      "Epoch 526 of 1000, Loss: 0.271, accuracy: 0.8388\n",
      "Epoch 527 of 1000, Loss: 0.23, accuracy: 0.8384\n",
      "Epoch 528 of 1000, Loss: 0.258, accuracy: 0.8385\n",
      "Epoch 529 of 1000, Loss: 0.446, accuracy: 0.8387\n",
      "Epoch 530 of 1000, Loss: 0.298, accuracy: 0.838\n",
      "Epoch 531 of 1000, Loss: 0.338, accuracy: 0.8385\n",
      "Epoch 532 of 1000, Loss: 0.581, accuracy: 0.839\n",
      "Epoch 533 of 1000, Loss: 0.41, accuracy: 0.8388\n",
      "Epoch 534 of 1000, Loss: 0.484, accuracy: 0.8386\n",
      "Epoch 535 of 1000, Loss: 0.417, accuracy: 0.8397\n",
      "Epoch 536 of 1000, Loss: 0.215, accuracy: 0.8387\n",
      "Epoch 537 of 1000, Loss: 0.589, accuracy: 0.8399\n",
      "Epoch 538 of 1000, Loss: 0.577, accuracy: 0.8385\n",
      "Epoch 539 of 1000, Loss: 0.303, accuracy: 0.8387\n",
      "Epoch 540 of 1000, Loss: 0.356, accuracy: 0.8392\n",
      "Epoch 541 of 1000, Loss: 0.396, accuracy: 0.8389\n",
      "Epoch 542 of 1000, Loss: 0.376, accuracy: 0.8383\n",
      "Epoch 543 of 1000, Loss: 0.213, accuracy: 0.8388\n",
      "Epoch 544 of 1000, Loss: 0.44, accuracy: 0.8385\n",
      "Epoch 545 of 1000, Loss: 0.31, accuracy: 0.8382\n",
      "Epoch 546 of 1000, Loss: 0.381, accuracy: 0.8397\n",
      "Epoch 547 of 1000, Loss: 0.296, accuracy: 0.839\n",
      "Epoch 548 of 1000, Loss: 0.395, accuracy: 0.8382\n",
      "Epoch 549 of 1000, Loss: 0.386, accuracy: 0.8391\n",
      "Epoch 550 of 1000, Loss: 0.284, accuracy: 0.8389\n",
      "Epoch 551 of 1000, Loss: 0.248, accuracy: 0.8388\n",
      "Epoch 552 of 1000, Loss: 0.31, accuracy: 0.8388\n",
      "Epoch 553 of 1000, Loss: 0.338, accuracy: 0.8381\n",
      "Epoch 554 of 1000, Loss: 0.339, accuracy: 0.8384\n",
      "Epoch 555 of 1000, Loss: 0.591, accuracy: 0.8378\n",
      "Epoch 556 of 1000, Loss: 0.302, accuracy: 0.839\n",
      "Epoch 557 of 1000, Loss: 0.329, accuracy: 0.839\n",
      "Epoch 558 of 1000, Loss: 0.183, accuracy: 0.8396\n",
      "Epoch 559 of 1000, Loss: 0.424, accuracy: 0.8391\n",
      "Epoch 560 of 1000, Loss: 0.482, accuracy: 0.8391\n",
      "Epoch 561 of 1000, Loss: 0.365, accuracy: 0.8396\n",
      "Epoch 562 of 1000, Loss: 0.495, accuracy: 0.8387\n",
      "Epoch 563 of 1000, Loss: 0.357, accuracy: 0.8386\n",
      "Epoch 564 of 1000, Loss: 0.834, accuracy: 0.8381\n",
      "Epoch 565 of 1000, Loss: 0.248, accuracy: 0.8385\n",
      "Epoch 566 of 1000, Loss: 0.442, accuracy: 0.839\n",
      "Epoch 567 of 1000, Loss: 0.261, accuracy: 0.8383\n",
      "Epoch 568 of 1000, Loss: 0.157, accuracy: 0.8402\n",
      "Epoch 569 of 1000, Loss: 0.151, accuracy: 0.8387\n",
      "Epoch 570 of 1000, Loss: 0.227, accuracy: 0.8387\n",
      "Epoch 571 of 1000, Loss: 0.273, accuracy: 0.839\n",
      "Epoch 572 of 1000, Loss: 0.349, accuracy: 0.8392\n",
      "Epoch 573 of 1000, Loss: 0.762, accuracy: 0.8392\n",
      "Epoch 574 of 1000, Loss: 0.178, accuracy: 0.8393\n",
      "Epoch 575 of 1000, Loss: 0.442, accuracy: 0.8381\n",
      "Epoch 576 of 1000, Loss: 0.253, accuracy: 0.8386\n",
      "Epoch 577 of 1000, Loss: 0.418, accuracy: 0.8392\n",
      "Epoch 578 of 1000, Loss: 0.329, accuracy: 0.8383\n",
      "Epoch 579 of 1000, Loss: 0.247, accuracy: 0.8393\n",
      "Epoch 580 of 1000, Loss: 0.213, accuracy: 0.8389\n",
      "Epoch 581 of 1000, Loss: 0.423, accuracy: 0.8382\n",
      "Epoch 582 of 1000, Loss: 0.689, accuracy: 0.8385\n",
      "Epoch 583 of 1000, Loss: 0.136, accuracy: 0.8385\n",
      "Epoch 584 of 1000, Loss: 0.157, accuracy: 0.8389\n",
      "Epoch 585 of 1000, Loss: 0.478, accuracy: 0.839\n",
      "Epoch 586 of 1000, Loss: 0.288, accuracy: 0.8395\n",
      "Epoch 587 of 1000, Loss: 0.289, accuracy: 0.8393\n",
      "Epoch 588 of 1000, Loss: 0.281, accuracy: 0.8391\n",
      "Epoch 589 of 1000, Loss: 0.472, accuracy: 0.8389\n",
      "Epoch 590 of 1000, Loss: 0.383, accuracy: 0.8387\n",
      "Epoch 591 of 1000, Loss: 0.294, accuracy: 0.8391\n",
      "Epoch 592 of 1000, Loss: 0.359, accuracy: 0.8391\n",
      "Epoch 593 of 1000, Loss: 0.221, accuracy: 0.8387\n",
      "Epoch 594 of 1000, Loss: 0.266, accuracy: 0.8383\n",
      "Epoch 595 of 1000, Loss: 0.311, accuracy: 0.8394\n",
      "Epoch 596 of 1000, Loss: 0.35, accuracy: 0.8396\n",
      "Epoch 597 of 1000, Loss: 0.396, accuracy: 0.8393\n",
      "Epoch 598 of 1000, Loss: 0.36, accuracy: 0.839\n",
      "Epoch 599 of 1000, Loss: 0.254, accuracy: 0.8396\n",
      "Epoch 600 of 1000, Loss: 0.464, accuracy: 0.8394\n",
      "Epoch 601 of 1000, Loss: 0.356, accuracy: 0.8383\n",
      "Epoch 602 of 1000, Loss: 0.328, accuracy: 0.8389\n",
      "Epoch 603 of 1000, Loss: 0.203, accuracy: 0.8386\n",
      "Epoch 604 of 1000, Loss: 0.248, accuracy: 0.8387\n",
      "Epoch 605 of 1000, Loss: 0.3, accuracy: 0.8381\n",
      "Epoch 606 of 1000, Loss: 0.516, accuracy: 0.8388\n",
      "Epoch 607 of 1000, Loss: 0.219, accuracy: 0.8398\n",
      "Epoch 608 of 1000, Loss: 0.247, accuracy: 0.8394\n",
      "Epoch 609 of 1000, Loss: 0.296, accuracy: 0.8389\n",
      "Epoch 610 of 1000, Loss: 0.641, accuracy: 0.8388\n",
      "Epoch 611 of 1000, Loss: 0.227, accuracy: 0.8385\n",
      "Epoch 612 of 1000, Loss: 0.212, accuracy: 0.8387\n",
      "Epoch 613 of 1000, Loss: 0.265, accuracy: 0.8387\n",
      "Epoch 614 of 1000, Loss: 0.168, accuracy: 0.8393\n",
      "Epoch 615 of 1000, Loss: 0.368, accuracy: 0.8387\n",
      "Epoch 616 of 1000, Loss: 0.336, accuracy: 0.8383\n",
      "Epoch 617 of 1000, Loss: 0.059, accuracy: 0.8373\n",
      "Epoch 618 of 1000, Loss: 0.137, accuracy: 0.8386\n",
      "Epoch 619 of 1000, Loss: 0.314, accuracy: 0.8391\n",
      "Epoch 620 of 1000, Loss: 0.238, accuracy: 0.8396\n",
      "Epoch 621 of 1000, Loss: 0.234, accuracy: 0.8386\n",
      "Epoch 622 of 1000, Loss: 0.316, accuracy: 0.8388\n",
      "Epoch 623 of 1000, Loss: 0.505, accuracy: 0.8391\n",
      "Epoch 624 of 1000, Loss: 0.302, accuracy: 0.8387\n",
      "Epoch 625 of 1000, Loss: 0.319, accuracy: 0.8389\n",
      "Epoch 626 of 1000, Loss: 0.248, accuracy: 0.8387\n",
      "Epoch 627 of 1000, Loss: 0.316, accuracy: 0.8386\n",
      "Epoch 628 of 1000, Loss: 0.159, accuracy: 0.839\n",
      "Epoch 629 of 1000, Loss: 0.528, accuracy: 0.8386\n",
      "Epoch 630 of 1000, Loss: 0.317, accuracy: 0.8384\n",
      "Epoch 631 of 1000, Loss: 0.348, accuracy: 0.8385\n",
      "Epoch 632 of 1000, Loss: 0.332, accuracy: 0.8389\n",
      "Epoch 633 of 1000, Loss: 0.438, accuracy: 0.8393\n",
      "Epoch 634 of 1000, Loss: 0.541, accuracy: 0.8391\n",
      "Epoch 635 of 1000, Loss: 0.211, accuracy: 0.8386\n",
      "Epoch 636 of 1000, Loss: 0.256, accuracy: 0.8391\n",
      "Epoch 637 of 1000, Loss: 0.183, accuracy: 0.8401\n",
      "Epoch 638 of 1000, Loss: 0.225, accuracy: 0.8389\n",
      "Epoch 639 of 1000, Loss: 0.29, accuracy: 0.8382\n",
      "Epoch 640 of 1000, Loss: 0.401, accuracy: 0.8393\n",
      "Epoch 641 of 1000, Loss: 0.22, accuracy: 0.8386\n",
      "Epoch 642 of 1000, Loss: 0.647, accuracy: 0.8388\n",
      "Epoch 643 of 1000, Loss: 0.216, accuracy: 0.8392\n",
      "Epoch 644 of 1000, Loss: 0.34, accuracy: 0.8387\n",
      "Epoch 645 of 1000, Loss: 0.39, accuracy: 0.8392\n",
      "Epoch 646 of 1000, Loss: 0.463, accuracy: 0.839\n",
      "Epoch 647 of 1000, Loss: 0.269, accuracy: 0.8381\n",
      "Epoch 648 of 1000, Loss: 0.587, accuracy: 0.8394\n",
      "Epoch 649 of 1000, Loss: 0.25, accuracy: 0.8379\n",
      "Epoch 650 of 1000, Loss: 0.414, accuracy: 0.838\n",
      "Epoch 651 of 1000, Loss: 0.197, accuracy: 0.8396\n",
      "Epoch 652 of 1000, Loss: 0.185, accuracy: 0.8379\n",
      "Epoch 653 of 1000, Loss: 0.36, accuracy: 0.8386\n",
      "Epoch 654 of 1000, Loss: 0.144, accuracy: 0.8382\n",
      "Epoch 655 of 1000, Loss: 0.374, accuracy: 0.839\n",
      "Epoch 656 of 1000, Loss: 0.381, accuracy: 0.8383\n",
      "Epoch 657 of 1000, Loss: 0.307, accuracy: 0.8392\n",
      "Epoch 658 of 1000, Loss: 0.387, accuracy: 0.8373\n",
      "Epoch 659 of 1000, Loss: 0.24, accuracy: 0.8394\n",
      "Epoch 660 of 1000, Loss: 0.088, accuracy: 0.8389\n",
      "Epoch 661 of 1000, Loss: 0.785, accuracy: 0.8388\n",
      "Epoch 662 of 1000, Loss: 0.274, accuracy: 0.8378\n",
      "Epoch 663 of 1000, Loss: 0.277, accuracy: 0.8387\n",
      "Epoch 664 of 1000, Loss: 0.302, accuracy: 0.8388\n",
      "Epoch 665 of 1000, Loss: 0.544, accuracy: 0.8386\n",
      "Epoch 666 of 1000, Loss: 0.272, accuracy: 0.838\n",
      "Epoch 667 of 1000, Loss: 0.197, accuracy: 0.8395\n",
      "Epoch 668 of 1000, Loss: 0.44, accuracy: 0.839\n",
      "Epoch 669 of 1000, Loss: 0.315, accuracy: 0.8397\n",
      "Epoch 670 of 1000, Loss: 0.227, accuracy: 0.8394\n",
      "Epoch 671 of 1000, Loss: 0.356, accuracy: 0.8379\n",
      "Epoch 672 of 1000, Loss: 0.231, accuracy: 0.8387\n",
      "Epoch 673 of 1000, Loss: 0.338, accuracy: 0.8386\n",
      "Epoch 674 of 1000, Loss: 0.34, accuracy: 0.839\n",
      "Epoch 675 of 1000, Loss: 0.094, accuracy: 0.8385\n",
      "Epoch 676 of 1000, Loss: 0.258, accuracy: 0.838\n",
      "Epoch 677 of 1000, Loss: 0.182, accuracy: 0.8392\n",
      "Epoch 678 of 1000, Loss: 0.187, accuracy: 0.8392\n",
      "Epoch 679 of 1000, Loss: 0.312, accuracy: 0.8391\n",
      "Epoch 680 of 1000, Loss: 0.398, accuracy: 0.8389\n",
      "Epoch 681 of 1000, Loss: 0.505, accuracy: 0.8377\n",
      "Epoch 682 of 1000, Loss: 0.266, accuracy: 0.8396\n",
      "Epoch 683 of 1000, Loss: 0.326, accuracy: 0.8378\n",
      "Epoch 684 of 1000, Loss: 0.669, accuracy: 0.8394\n",
      "Epoch 685 of 1000, Loss: 0.225, accuracy: 0.838\n",
      "Epoch 686 of 1000, Loss: 0.283, accuracy: 0.838\n",
      "Epoch 687 of 1000, Loss: 0.638, accuracy: 0.8387\n",
      "Epoch 688 of 1000, Loss: 0.401, accuracy: 0.8381\n",
      "Epoch 689 of 1000, Loss: 0.396, accuracy: 0.839\n",
      "Epoch 690 of 1000, Loss: 0.346, accuracy: 0.8393\n",
      "Epoch 691 of 1000, Loss: 0.421, accuracy: 0.8387\n",
      "Epoch 692 of 1000, Loss: 0.205, accuracy: 0.8389\n",
      "Epoch 693 of 1000, Loss: 0.283, accuracy: 0.8391\n",
      "Epoch 694 of 1000, Loss: 0.075, accuracy: 0.8377\n",
      "Epoch 695 of 1000, Loss: 0.353, accuracy: 0.8388\n",
      "Epoch 696 of 1000, Loss: 0.163, accuracy: 0.839\n",
      "Epoch 697 of 1000, Loss: 0.411, accuracy: 0.838\n",
      "Epoch 698 of 1000, Loss: 0.527, accuracy: 0.8385\n",
      "Epoch 699 of 1000, Loss: 0.461, accuracy: 0.8384\n",
      "Epoch 700 of 1000, Loss: 0.081, accuracy: 0.8388\n",
      "Epoch 701 of 1000, Loss: 0.683, accuracy: 0.839\n",
      "Epoch 702 of 1000, Loss: 0.329, accuracy: 0.8384\n",
      "Epoch 703 of 1000, Loss: 0.448, accuracy: 0.839\n",
      "Epoch 704 of 1000, Loss: 0.292, accuracy: 0.8383\n",
      "Epoch 705 of 1000, Loss: 0.262, accuracy: 0.8388\n",
      "Epoch 706 of 1000, Loss: 0.364, accuracy: 0.8386\n",
      "Epoch 707 of 1000, Loss: 0.069, accuracy: 0.8391\n",
      "Epoch 708 of 1000, Loss: 0.31, accuracy: 0.8386\n",
      "Epoch 709 of 1000, Loss: 0.251, accuracy: 0.8395\n",
      "Epoch 710 of 1000, Loss: 0.382, accuracy: 0.8393\n",
      "Epoch 711 of 1000, Loss: 0.186, accuracy: 0.8383\n",
      "Epoch 712 of 1000, Loss: 0.09, accuracy: 0.8389\n",
      "Epoch 713 of 1000, Loss: 0.78, accuracy: 0.8396\n",
      "Epoch 714 of 1000, Loss: 0.409, accuracy: 0.8391\n",
      "Epoch 715 of 1000, Loss: 0.403, accuracy: 0.8376\n",
      "Epoch 716 of 1000, Loss: 0.167, accuracy: 0.8386\n",
      "Epoch 717 of 1000, Loss: 0.491, accuracy: 0.8384\n",
      "Epoch 718 of 1000, Loss: 0.234, accuracy: 0.8385\n",
      "Epoch 719 of 1000, Loss: 0.385, accuracy: 0.8392\n",
      "Epoch 720 of 1000, Loss: 0.163, accuracy: 0.8379\n",
      "Epoch 721 of 1000, Loss: 0.301, accuracy: 0.8388\n",
      "Epoch 722 of 1000, Loss: 0.296, accuracy: 0.8391\n",
      "Epoch 723 of 1000, Loss: 0.196, accuracy: 0.8394\n",
      "Epoch 724 of 1000, Loss: 0.258, accuracy: 0.8392\n",
      "Epoch 725 of 1000, Loss: 0.747, accuracy: 0.8399\n",
      "Epoch 726 of 1000, Loss: 0.176, accuracy: 0.8396\n",
      "Epoch 727 of 1000, Loss: 0.259, accuracy: 0.8385\n",
      "Epoch 728 of 1000, Loss: 0.299, accuracy: 0.8387\n",
      "Epoch 729 of 1000, Loss: 0.411, accuracy: 0.8388\n",
      "Epoch 730 of 1000, Loss: 0.283, accuracy: 0.8384\n",
      "Epoch 731 of 1000, Loss: 0.843, accuracy: 0.8384\n",
      "Epoch 732 of 1000, Loss: 0.374, accuracy: 0.8383\n",
      "Epoch 733 of 1000, Loss: 0.172, accuracy: 0.8391\n",
      "Epoch 734 of 1000, Loss: 0.338, accuracy: 0.8392\n",
      "Epoch 735 of 1000, Loss: 0.282, accuracy: 0.8386\n",
      "Epoch 736 of 1000, Loss: 0.222, accuracy: 0.8386\n",
      "Epoch 737 of 1000, Loss: 0.392, accuracy: 0.8395\n",
      "Epoch 738 of 1000, Loss: 0.23, accuracy: 0.839\n",
      "Epoch 739 of 1000, Loss: 0.278, accuracy: 0.8391\n",
      "Epoch 740 of 1000, Loss: 0.3, accuracy: 0.8394\n",
      "Epoch 741 of 1000, Loss: 0.408, accuracy: 0.8386\n",
      "Epoch 742 of 1000, Loss: 0.369, accuracy: 0.8383\n",
      "Epoch 743 of 1000, Loss: 0.169, accuracy: 0.8385\n",
      "Epoch 744 of 1000, Loss: 0.308, accuracy: 0.8383\n",
      "Epoch 745 of 1000, Loss: 0.316, accuracy: 0.8391\n",
      "Epoch 746 of 1000, Loss: 0.444, accuracy: 0.838\n",
      "Epoch 747 of 1000, Loss: 0.709, accuracy: 0.8393\n",
      "Epoch 748 of 1000, Loss: 0.241, accuracy: 0.8393\n",
      "Epoch 749 of 1000, Loss: 0.175, accuracy: 0.8383\n",
      "Epoch 750 of 1000, Loss: 0.43, accuracy: 0.8385\n",
      "Epoch 751 of 1000, Loss: 0.077, accuracy: 0.84\n",
      "Epoch 752 of 1000, Loss: 0.525, accuracy: 0.8387\n",
      "Epoch 753 of 1000, Loss: 0.324, accuracy: 0.8385\n",
      "Epoch 754 of 1000, Loss: 0.482, accuracy: 0.8383\n",
      "Epoch 755 of 1000, Loss: 0.343, accuracy: 0.8384\n",
      "Epoch 756 of 1000, Loss: 0.273, accuracy: 0.8387\n",
      "Epoch 757 of 1000, Loss: 0.16, accuracy: 0.8383\n",
      "Epoch 758 of 1000, Loss: 0.693, accuracy: 0.8386\n",
      "Epoch 759 of 1000, Loss: 0.182, accuracy: 0.8382\n",
      "Epoch 760 of 1000, Loss: 0.353, accuracy: 0.8399\n",
      "Epoch 761 of 1000, Loss: 0.461, accuracy: 0.839\n",
      "Epoch 762 of 1000, Loss: 0.492, accuracy: 0.8384\n",
      "Epoch 763 of 1000, Loss: 0.645, accuracy: 0.8385\n",
      "Epoch 764 of 1000, Loss: 0.473, accuracy: 0.8387\n",
      "Epoch 765 of 1000, Loss: 0.34, accuracy: 0.8397\n",
      "Epoch 766 of 1000, Loss: 0.068, accuracy: 0.8402\n",
      "Epoch 767 of 1000, Loss: 0.214, accuracy: 0.839\n",
      "Epoch 768 of 1000, Loss: 0.167, accuracy: 0.8377\n",
      "Epoch 769 of 1000, Loss: 0.618, accuracy: 0.8384\n",
      "Epoch 770 of 1000, Loss: 0.261, accuracy: 0.838\n",
      "Epoch 771 of 1000, Loss: 0.211, accuracy: 0.8395\n",
      "Epoch 772 of 1000, Loss: 0.194, accuracy: 0.8385\n",
      "Epoch 773 of 1000, Loss: 0.251, accuracy: 0.8383\n",
      "Epoch 774 of 1000, Loss: 0.341, accuracy: 0.8386\n",
      "Epoch 775 of 1000, Loss: 0.183, accuracy: 0.8387\n",
      "Epoch 776 of 1000, Loss: 0.242, accuracy: 0.8382\n",
      "Epoch 777 of 1000, Loss: 0.261, accuracy: 0.8394\n",
      "Epoch 778 of 1000, Loss: 0.283, accuracy: 0.8385\n",
      "Epoch 779 of 1000, Loss: 0.45, accuracy: 0.8395\n",
      "Epoch 780 of 1000, Loss: 0.33, accuracy: 0.8394\n",
      "Epoch 781 of 1000, Loss: 0.252, accuracy: 0.8377\n",
      "Epoch 782 of 1000, Loss: 0.262, accuracy: 0.8386\n",
      "Epoch 783 of 1000, Loss: 0.086, accuracy: 0.839\n",
      "Epoch 784 of 1000, Loss: 0.217, accuracy: 0.8391\n",
      "Epoch 785 of 1000, Loss: 0.392, accuracy: 0.839\n",
      "Epoch 786 of 1000, Loss: 0.142, accuracy: 0.8399\n",
      "Epoch 787 of 1000, Loss: 0.436, accuracy: 0.8387\n",
      "Epoch 788 of 1000, Loss: 0.359, accuracy: 0.8378\n",
      "Epoch 789 of 1000, Loss: 0.402, accuracy: 0.8385\n",
      "Epoch 790 of 1000, Loss: 0.386, accuracy: 0.838\n",
      "Epoch 791 of 1000, Loss: 0.391, accuracy: 0.8396\n",
      "Epoch 792 of 1000, Loss: 0.468, accuracy: 0.8386\n",
      "Epoch 793 of 1000, Loss: 0.319, accuracy: 0.8395\n",
      "Epoch 794 of 1000, Loss: 0.385, accuracy: 0.8384\n",
      "Epoch 795 of 1000, Loss: 0.111, accuracy: 0.8382\n",
      "Epoch 796 of 1000, Loss: 0.304, accuracy: 0.838\n",
      "Epoch 797 of 1000, Loss: 0.493, accuracy: 0.8384\n",
      "Epoch 798 of 1000, Loss: 0.289, accuracy: 0.839\n",
      "Epoch 799 of 1000, Loss: 0.423, accuracy: 0.8376\n",
      "Epoch 800 of 1000, Loss: 0.466, accuracy: 0.838\n",
      "Epoch 801 of 1000, Loss: 0.513, accuracy: 0.8385\n",
      "Epoch 802 of 1000, Loss: 0.188, accuracy: 0.8391\n",
      "Epoch 803 of 1000, Loss: 0.171, accuracy: 0.8391\n",
      "Epoch 804 of 1000, Loss: 0.206, accuracy: 0.8387\n",
      "Epoch 805 of 1000, Loss: 0.472, accuracy: 0.8391\n",
      "Epoch 806 of 1000, Loss: 0.233, accuracy: 0.8391\n",
      "Epoch 807 of 1000, Loss: 0.141, accuracy: 0.8389\n",
      "Epoch 808 of 1000, Loss: 0.468, accuracy: 0.839\n",
      "Epoch 809 of 1000, Loss: 0.381, accuracy: 0.8386\n",
      "Epoch 810 of 1000, Loss: 0.379, accuracy: 0.8396\n",
      "Epoch 811 of 1000, Loss: 0.244, accuracy: 0.8383\n",
      "Epoch 812 of 1000, Loss: 0.306, accuracy: 0.8383\n",
      "Epoch 813 of 1000, Loss: 0.33, accuracy: 0.8385\n",
      "Epoch 814 of 1000, Loss: 0.228, accuracy: 0.8378\n",
      "Epoch 815 of 1000, Loss: 0.358, accuracy: 0.8398\n",
      "Epoch 816 of 1000, Loss: 0.307, accuracy: 0.838\n",
      "Epoch 817 of 1000, Loss: 0.366, accuracy: 0.8394\n",
      "Epoch 818 of 1000, Loss: 0.466, accuracy: 0.8382\n",
      "Epoch 819 of 1000, Loss: 0.263, accuracy: 0.839\n",
      "Epoch 820 of 1000, Loss: 0.479, accuracy: 0.84\n",
      "Epoch 821 of 1000, Loss: 0.573, accuracy: 0.8378\n",
      "Epoch 822 of 1000, Loss: 0.549, accuracy: 0.8394\n",
      "Epoch 823 of 1000, Loss: 0.351, accuracy: 0.8385\n",
      "Epoch 824 of 1000, Loss: 0.44, accuracy: 0.8397\n",
      "Epoch 825 of 1000, Loss: 0.506, accuracy: 0.8393\n",
      "Epoch 826 of 1000, Loss: 0.615, accuracy: 0.8378\n",
      "Epoch 827 of 1000, Loss: 0.567, accuracy: 0.839\n",
      "Epoch 828 of 1000, Loss: 0.306, accuracy: 0.839\n",
      "Epoch 829 of 1000, Loss: 0.442, accuracy: 0.839\n",
      "Epoch 830 of 1000, Loss: 0.431, accuracy: 0.8378\n",
      "Epoch 831 of 1000, Loss: 0.379, accuracy: 0.8394\n",
      "Epoch 832 of 1000, Loss: 0.295, accuracy: 0.8384\n",
      "Epoch 833 of 1000, Loss: 0.391, accuracy: 0.8379\n",
      "Epoch 834 of 1000, Loss: 0.423, accuracy: 0.8388\n",
      "Epoch 835 of 1000, Loss: 0.135, accuracy: 0.8386\n",
      "Epoch 836 of 1000, Loss: 0.375, accuracy: 0.8387\n",
      "Epoch 837 of 1000, Loss: 0.58, accuracy: 0.8393\n",
      "Epoch 838 of 1000, Loss: 0.375, accuracy: 0.8389\n",
      "Epoch 839 of 1000, Loss: 0.514, accuracy: 0.8385\n",
      "Epoch 840 of 1000, Loss: 0.496, accuracy: 0.838\n",
      "Epoch 841 of 1000, Loss: 0.362, accuracy: 0.8377\n",
      "Epoch 842 of 1000, Loss: 0.504, accuracy: 0.8379\n",
      "Epoch 843 of 1000, Loss: 0.178, accuracy: 0.839\n",
      "Epoch 844 of 1000, Loss: 0.126, accuracy: 0.8383\n",
      "Epoch 845 of 1000, Loss: 0.468, accuracy: 0.839\n",
      "Epoch 846 of 1000, Loss: 0.261, accuracy: 0.8393\n",
      "Epoch 847 of 1000, Loss: 0.201, accuracy: 0.8393\n",
      "Epoch 848 of 1000, Loss: 0.292, accuracy: 0.8385\n",
      "Epoch 849 of 1000, Loss: 0.33, accuracy: 0.8391\n",
      "Epoch 850 of 1000, Loss: 0.281, accuracy: 0.8397\n",
      "Epoch 851 of 1000, Loss: 0.445, accuracy: 0.8395\n",
      "Epoch 852 of 1000, Loss: 0.089, accuracy: 0.8392\n",
      "Epoch 853 of 1000, Loss: 0.083, accuracy: 0.8393\n",
      "Epoch 854 of 1000, Loss: 0.38, accuracy: 0.8393\n",
      "Epoch 855 of 1000, Loss: 0.293, accuracy: 0.8393\n",
      "Epoch 856 of 1000, Loss: 0.297, accuracy: 0.8383\n",
      "Epoch 857 of 1000, Loss: 0.192, accuracy: 0.8386\n",
      "Epoch 858 of 1000, Loss: 0.221, accuracy: 0.8385\n",
      "Epoch 859 of 1000, Loss: 0.223, accuracy: 0.8384\n",
      "Epoch 860 of 1000, Loss: 0.783, accuracy: 0.8385\n",
      "Epoch 861 of 1000, Loss: 0.342, accuracy: 0.8376\n",
      "Epoch 862 of 1000, Loss: 0.169, accuracy: 0.839\n",
      "Epoch 863 of 1000, Loss: 0.265, accuracy: 0.8392\n",
      "Epoch 864 of 1000, Loss: 0.086, accuracy: 0.8391\n",
      "Epoch 865 of 1000, Loss: 0.261, accuracy: 0.8385\n",
      "Epoch 866 of 1000, Loss: 0.281, accuracy: 0.8392\n",
      "Epoch 867 of 1000, Loss: 0.239, accuracy: 0.8392\n",
      "Epoch 868 of 1000, Loss: 0.431, accuracy: 0.8378\n",
      "Epoch 869 of 1000, Loss: 0.205, accuracy: 0.8393\n",
      "Epoch 870 of 1000, Loss: 0.425, accuracy: 0.8385\n",
      "Epoch 871 of 1000, Loss: 0.314, accuracy: 0.839\n",
      "Epoch 872 of 1000, Loss: 0.45, accuracy: 0.838\n",
      "Epoch 873 of 1000, Loss: 0.309, accuracy: 0.8384\n",
      "Epoch 874 of 1000, Loss: 0.256, accuracy: 0.8397\n",
      "Epoch 875 of 1000, Loss: 0.347, accuracy: 0.8387\n",
      "Epoch 876 of 1000, Loss: 0.303, accuracy: 0.8383\n",
      "Epoch 877 of 1000, Loss: 0.254, accuracy: 0.8394\n",
      "Epoch 878 of 1000, Loss: 0.469, accuracy: 0.8384\n",
      "Epoch 879 of 1000, Loss: 0.183, accuracy: 0.8382\n",
      "Epoch 880 of 1000, Loss: 0.403, accuracy: 0.8385\n",
      "Epoch 881 of 1000, Loss: 0.446, accuracy: 0.8392\n",
      "Epoch 882 of 1000, Loss: 0.172, accuracy: 0.8386\n",
      "Epoch 883 of 1000, Loss: 0.3, accuracy: 0.8392\n",
      "Epoch 884 of 1000, Loss: 0.31, accuracy: 0.8387\n",
      "Epoch 885 of 1000, Loss: 0.413, accuracy: 0.8382\n",
      "Epoch 886 of 1000, Loss: 0.21, accuracy: 0.8387\n",
      "Epoch 887 of 1000, Loss: 0.531, accuracy: 0.8396\n",
      "Epoch 888 of 1000, Loss: 0.264, accuracy: 0.8386\n",
      "Epoch 889 of 1000, Loss: 0.237, accuracy: 0.8387\n",
      "Epoch 890 of 1000, Loss: 0.357, accuracy: 0.8388\n",
      "Epoch 891 of 1000, Loss: 0.24, accuracy: 0.8378\n",
      "Epoch 892 of 1000, Loss: 0.351, accuracy: 0.8392\n",
      "Epoch 893 of 1000, Loss: 0.42, accuracy: 0.8384\n",
      "Epoch 894 of 1000, Loss: 0.267, accuracy: 0.8387\n",
      "Epoch 895 of 1000, Loss: 0.413, accuracy: 0.8386\n",
      "Epoch 896 of 1000, Loss: 0.379, accuracy: 0.8383\n",
      "Epoch 897 of 1000, Loss: 0.544, accuracy: 0.8396\n",
      "Epoch 898 of 1000, Loss: 0.117, accuracy: 0.839\n",
      "Epoch 899 of 1000, Loss: 0.468, accuracy: 0.8387\n",
      "Epoch 900 of 1000, Loss: 0.233, accuracy: 0.8391\n",
      "Epoch 901 of 1000, Loss: 0.778, accuracy: 0.8387\n",
      "Epoch 902 of 1000, Loss: 0.285, accuracy: 0.8388\n",
      "Epoch 903 of 1000, Loss: 0.24, accuracy: 0.839\n",
      "Epoch 904 of 1000, Loss: 0.548, accuracy: 0.8382\n",
      "Epoch 905 of 1000, Loss: 0.324, accuracy: 0.8396\n",
      "Epoch 906 of 1000, Loss: 0.399, accuracy: 0.838\n",
      "Epoch 907 of 1000, Loss: 0.482, accuracy: 0.8391\n",
      "Epoch 908 of 1000, Loss: 0.201, accuracy: 0.838\n",
      "Epoch 909 of 1000, Loss: 0.157, accuracy: 0.8387\n",
      "Epoch 910 of 1000, Loss: 0.582, accuracy: 0.8389\n",
      "Epoch 911 of 1000, Loss: 0.178, accuracy: 0.84\n",
      "Epoch 912 of 1000, Loss: 0.107, accuracy: 0.838\n",
      "Epoch 913 of 1000, Loss: 0.256, accuracy: 0.8382\n",
      "Epoch 914 of 1000, Loss: 0.415, accuracy: 0.8397\n",
      "Epoch 915 of 1000, Loss: 0.725, accuracy: 0.8392\n",
      "Epoch 916 of 1000, Loss: 0.481, accuracy: 0.8395\n",
      "Epoch 917 of 1000, Loss: 0.339, accuracy: 0.8394\n",
      "Epoch 918 of 1000, Loss: 0.163, accuracy: 0.8387\n",
      "Epoch 919 of 1000, Loss: 0.416, accuracy: 0.8383\n",
      "Epoch 920 of 1000, Loss: 0.217, accuracy: 0.8395\n",
      "Epoch 921 of 1000, Loss: 0.333, accuracy: 0.8394\n",
      "Epoch 922 of 1000, Loss: 0.341, accuracy: 0.8383\n",
      "Epoch 923 of 1000, Loss: 0.357, accuracy: 0.839\n",
      "Epoch 924 of 1000, Loss: 0.612, accuracy: 0.8391\n",
      "Epoch 925 of 1000, Loss: 0.297, accuracy: 0.8392\n",
      "Epoch 926 of 1000, Loss: 0.359, accuracy: 0.8388\n",
      "Epoch 927 of 1000, Loss: 0.275, accuracy: 0.8383\n",
      "Epoch 928 of 1000, Loss: 0.211, accuracy: 0.8386\n",
      "Epoch 929 of 1000, Loss: 0.367, accuracy: 0.8396\n",
      "Epoch 930 of 1000, Loss: 0.353, accuracy: 0.8388\n",
      "Epoch 931 of 1000, Loss: 0.333, accuracy: 0.8387\n",
      "Epoch 932 of 1000, Loss: 0.276, accuracy: 0.8394\n",
      "Epoch 933 of 1000, Loss: 0.611, accuracy: 0.8391\n",
      "Epoch 934 of 1000, Loss: 0.387, accuracy: 0.8393\n",
      "Epoch 935 of 1000, Loss: 0.277, accuracy: 0.8387\n",
      "Epoch 936 of 1000, Loss: 0.701, accuracy: 0.8384\n",
      "Epoch 937 of 1000, Loss: 0.351, accuracy: 0.8389\n",
      "Epoch 938 of 1000, Loss: 0.09, accuracy: 0.8386\n",
      "Epoch 939 of 1000, Loss: 0.455, accuracy: 0.8394\n",
      "Epoch 940 of 1000, Loss: 0.16, accuracy: 0.8377\n",
      "Epoch 941 of 1000, Loss: 0.395, accuracy: 0.8392\n",
      "Epoch 942 of 1000, Loss: 0.339, accuracy: 0.8388\n",
      "Epoch 943 of 1000, Loss: 0.603, accuracy: 0.8383\n",
      "Epoch 944 of 1000, Loss: 0.334, accuracy: 0.8387\n",
      "Epoch 945 of 1000, Loss: 0.573, accuracy: 0.8386\n",
      "Epoch 946 of 1000, Loss: 0.529, accuracy: 0.8392\n",
      "Epoch 947 of 1000, Loss: 0.279, accuracy: 0.8391\n",
      "Epoch 948 of 1000, Loss: 0.617, accuracy: 0.8398\n",
      "Epoch 949 of 1000, Loss: 0.274, accuracy: 0.8377\n",
      "Epoch 950 of 1000, Loss: 0.359, accuracy: 0.8381\n",
      "Epoch 951 of 1000, Loss: 0.618, accuracy: 0.8378\n",
      "Epoch 952 of 1000, Loss: 0.193, accuracy: 0.8387\n",
      "Epoch 953 of 1000, Loss: 0.365, accuracy: 0.8388\n",
      "Epoch 954 of 1000, Loss: 0.273, accuracy: 0.8391\n",
      "Epoch 955 of 1000, Loss: 0.397, accuracy: 0.8399\n",
      "Epoch 956 of 1000, Loss: 0.409, accuracy: 0.8389\n",
      "Epoch 957 of 1000, Loss: 0.332, accuracy: 0.8392\n",
      "Epoch 958 of 1000, Loss: 0.336, accuracy: 0.8387\n",
      "Epoch 959 of 1000, Loss: 0.459, accuracy: 0.8395\n",
      "Epoch 960 of 1000, Loss: 0.404, accuracy: 0.8387\n",
      "Epoch 961 of 1000, Loss: 0.171, accuracy: 0.8391\n",
      "Epoch 962 of 1000, Loss: 0.519, accuracy: 0.8393\n",
      "Epoch 963 of 1000, Loss: 0.554, accuracy: 0.8397\n",
      "Epoch 964 of 1000, Loss: 0.173, accuracy: 0.839\n",
      "Epoch 965 of 1000, Loss: 0.454, accuracy: 0.8392\n",
      "Epoch 966 of 1000, Loss: 0.115, accuracy: 0.8384\n",
      "Epoch 967 of 1000, Loss: 0.13, accuracy: 0.8383\n",
      "Epoch 968 of 1000, Loss: 0.205, accuracy: 0.8397\n",
      "Epoch 969 of 1000, Loss: 0.292, accuracy: 0.8398\n",
      "Epoch 970 of 1000, Loss: 0.242, accuracy: 0.8382\n",
      "Epoch 971 of 1000, Loss: 0.569, accuracy: 0.8398\n",
      "Epoch 972 of 1000, Loss: 0.292, accuracy: 0.8391\n",
      "Epoch 973 of 1000, Loss: 0.369, accuracy: 0.839\n",
      "Epoch 974 of 1000, Loss: 0.251, accuracy: 0.8383\n",
      "Epoch 975 of 1000, Loss: 0.267, accuracy: 0.8383\n",
      "Epoch 976 of 1000, Loss: 0.302, accuracy: 0.8393\n",
      "Epoch 977 of 1000, Loss: 0.55, accuracy: 0.8394\n",
      "Epoch 978 of 1000, Loss: 0.457, accuracy: 0.8383\n",
      "Epoch 979 of 1000, Loss: 0.106, accuracy: 0.8383\n",
      "Epoch 980 of 1000, Loss: 0.321, accuracy: 0.8392\n",
      "Epoch 981 of 1000, Loss: 0.058, accuracy: 0.8388\n",
      "Epoch 982 of 1000, Loss: 0.482, accuracy: 0.8383\n",
      "Epoch 983 of 1000, Loss: 0.342, accuracy: 0.8385\n",
      "Epoch 984 of 1000, Loss: 0.419, accuracy: 0.8383\n",
      "Epoch 985 of 1000, Loss: 0.369, accuracy: 0.8381\n",
      "Epoch 986 of 1000, Loss: 0.211, accuracy: 0.8389\n",
      "Epoch 987 of 1000, Loss: 0.258, accuracy: 0.8396\n",
      "Epoch 988 of 1000, Loss: 0.551, accuracy: 0.8379\n",
      "Epoch 989 of 1000, Loss: 0.133, accuracy: 0.8385\n",
      "Epoch 990 of 1000, Loss: 0.395, accuracy: 0.8389\n",
      "Epoch 991 of 1000, Loss: 0.499, accuracy: 0.8399\n",
      "Epoch 992 of 1000, Loss: 0.448, accuracy: 0.8377\n",
      "Epoch 993 of 1000, Loss: 0.327, accuracy: 0.8391\n",
      "Epoch 994 of 1000, Loss: 0.323, accuracy: 0.8386\n",
      "Epoch 995 of 1000, Loss: 0.255, accuracy: 0.839\n",
      "Epoch 996 of 1000, Loss: 0.242, accuracy: 0.8387\n",
      "Epoch 997 of 1000, Loss: 0.143, accuracy: 0.8388\n",
      "Epoch 998 of 1000, Loss: 0.23, accuracy: 0.8385\n",
      "Epoch 999 of 1000, Loss: 0.391, accuracy: 0.8403\n",
      "Epoch 1000 of 1000, Loss: 0.262, accuracy: 0.8387\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(wide_model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 50\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "# from http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_w = Variable(batch[:, 1:]).float()\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_model(X_w)\n",
    "        loss = F.binary_cross_entropy(y_pred, y.reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct += float((y_pred_cat == y).sum().item())\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "                                                           n_epochs, round(loss.item(),3), round(correct/total,4)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Deep Part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('education', 16, 10), ('relationship', 6, 8), ('occupation', 15, 10), ('workclass', 9, 10), ('native-country', 42, 10)]\n",
      "{'education': 0, 'relationship': 1, 'workclass': 2, 'occupation': 3, 'native-country': 4, 'age': 5, 'hours-per-week': 6}\n"
     ]
    }
   ],
   "source": [
    "print(wd_dataset['embeddings_input'])\n",
    "print(wd_dataset['deep_column_idx'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(16, 10)\n"
     ]
    }
   ],
   "source": [
    "col_name, unique_vals, n_emb = wd_dataset['embeddings_input'][0]\n",
    "emb_layer = nn.Embedding(unique_vals, n_emb)\n",
    "print(emb_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep-side, which consists in a series of embeddings and numerical\n",
    "    features passed through a series of dense layers.\n",
    "\n",
    "    Params:\n",
    "    --------\n",
    "    embeddings_input (tuple): 3-elements tuple with the embeddings \"set-up\" -\n",
    "    (col_name, unique_values, embeddings dim)\n",
    "    continuous_cols (list) : list with the name of the continuum columns\n",
    "    deep_column_idx (dict) : dictionary where the keys are column names and the values\n",
    "    their corresponding index in the deep-side input tensor\n",
    "    hidden_layers (list) : list with the number of units per hidden layer\n",
    "    n_class (int) : number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,n_class):\n",
    "\n",
    "        super(Deep, self).__init__()\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        # build the embeddings that will be passed through the deep side\n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        # the input dimension to the 1st hidden layer will be the sum of the\n",
    "        # embeddings dimensions plus the number of continuous features\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        for i,h in enumerate(self.hidden_layers[1:],1):\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1], n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X[:,self.deep_column_idx[col]].long())\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        out = torch.sigmoid(self.output(x_deep))\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "hidden_layers = [100,50]\n",
    "deep_model = Deep(embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep(\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 8)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (emb_layer_native-country): Embedding(42, 10)\n",
      "  (linear_1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (linear_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(deep_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  3.        ,  5.        , ...,  0.        ,\n        -1.14100392, -0.03408696],\n       [ 0.        , 11.        ,  0.        , ...,  0.        ,\n        -1.14100392, -0.03408696],\n       [ 1.        ,  9.        ,  1.        , ..., 27.        ,\n         0.24480847,  0.77292975],\n       ...,\n       [ 0.        ,  7.        ,  2.        , ...,  3.        ,\n        -0.92219144, -2.45513709],\n       [ 0.        ,  3.        ,  1.        , ...,  0.        ,\n         0.24480847,  2.06415648],\n       [ 0.        ,  3.        ,  0.        , ...,  0.        ,\n        -0.48456647, -2.05162874]])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].deep])\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoziqi/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.334, accuracy: 0.8242\n",
      "Epoch 2 of 10, Loss: 0.148, accuracy: 0.839\n",
      "Epoch 3 of 10, Loss: 0.177, accuracy: 0.8415\n",
      "Epoch 4 of 10, Loss: 0.16, accuracy: 0.8433\n",
      "Epoch 5 of 10, Loss: 0.137, accuracy: 0.844\n",
      "Epoch 6 of 10, Loss: 0.286, accuracy: 0.844\n",
      "Epoch 7 of 10, Loss: 0.439, accuracy: 0.8456\n",
      "Epoch 8 of 10, Loss: 0.114, accuracy: 0.8462\n",
      "Epoch 9 of 10, Loss: 0.237, accuracy: 0.8448\n",
      "Epoch 10 of 10, Loss: 0.561, accuracy: 0.8456\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(deep_model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_d = Variable(batch[:, 1:])\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = deep_model(X_d)\n",
    "        loss = F.binary_cross_entropy(y_pred, y.reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct+= float((y_pred_cat == y).sum().item())\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "                                                           n_epochs, round(loss.item(),3), round(correct/total,4)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combine the two parts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class WideDeep(nn.Module):\n",
    "\n",
    "    def __init__(self, wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class):\n",
    "\n",
    "        super(WideDeep, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        for i,h in enumerate(self.hidden_layers[1:],1):\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1]+self.wide_dim, n_class)\n",
    "\n",
    "    def forward(self, X_w, X_d):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X_d[:,self.deep_column_idx[col]].long())\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X_d[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        wide_deep_input = torch.cat([x_deep, X_w.float()], 1)\n",
    "\n",
    "        out = torch.sigmoid(self.output(wide_deep_input))\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "WideDeep(\n  (emb_layer_education): Embedding(16, 10)\n  (emb_layer_relationship): Embedding(6, 8)\n  (emb_layer_occupation): Embedding(15, 10)\n  (emb_layer_workclass): Embedding(9, 10)\n  (emb_layer_native-country): Embedding(42, 10)\n  (linear_1): Linear(in_features=50, out_features=100, bias=True)\n  (linear_2): Linear(in_features=100, out_features=50, bias=True)\n  (output): Linear(in_features=848, out_features=1, bias=True)\n)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_deep_model = WideDeep(wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)\n",
    "wide_deep_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class WideDeepLoader(Dataset):\n",
    "    \"\"\"Helper to facilitate loading the data to the pytorch models.\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    data: namedtuple with 3 elements - (wide_input_data, deep_inp_data, target)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.X_wide = data.wide\n",
    "        self.X_deep = data.deep\n",
    "        self.Y = data.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        xw = self.X_wide[idx]\n",
    "        xd = self.X_deep[idx]\n",
    "        y  = self.Y[idx]\n",
    "\n",
    "        return xw, xd, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "\n",
    "train_dataset = wd_dataset['train_dataset']\n",
    "widedeep_dataset = WideDeepLoader(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=widedeep_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoziqi/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.184, accuracy: 0.8248\n",
      "Epoch 2 of 10, Loss: 0.213, accuracy: 0.836\n",
      "Epoch 3 of 10, Loss: 0.361, accuracy: 0.8395\n",
      "Epoch 4 of 10, Loss: 0.269, accuracy: 0.8399\n",
      "Epoch 5 of 10, Loss: 0.32, accuracy: 0.8419\n",
      "Epoch 6 of 10, Loss: 0.232, accuracy: 0.843\n",
      "Epoch 7 of 10, Loss: 0.264, accuracy: 0.8428\n",
      "Epoch 8 of 10, Loss: 0.351, accuracy: 0.8417\n",
      "Epoch 9 of 10, Loss: 0.3, accuracy: 0.845\n",
      "Epoch 10 of 10, Loss: 0.183, accuracy: 0.8438\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(wide_deep_model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, (X_wide, X_deep, target) in enumerate(train_loader):\n",
    "        X_d = Variable(X_deep)\n",
    "        X_w = Variable(X_wide)\n",
    "        y = Variable(target).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_deep_model(X_w, X_d)\n",
    "        loss = F.binary_cross_entropy(y_pred, y.reshape(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct+= float((y_pred_cat == y).sum().item())\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "                                                           n_epochs, round(loss.item(),3), round(correct/total,4)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}